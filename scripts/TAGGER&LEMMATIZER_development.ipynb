{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pyconll\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "import sddk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sciencedata.dk username (format '123456@au.dk'): 648597@au.dk\n",
      "sciencedata.dk password: ········\n",
      "connection with shared folder established with you as its owner\n",
      "endpoint variable has been configured to: https://sciencedata.dk/files/SDAM_root/\n"
     ]
    }
   ],
   "source": [
    "conf = sddk.configure(\"SDAM_root\", \"648597@au.dk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[38;5;2m✔ Successfully created model\u001b[0m\n",
      "134209it [00:06, 20760.69it/s]/word2vec_win2.txt\n",
      "\u001b[2K\u001b[38;5;2m✔ Loaded vectors from ../data/word2vec_win2.txt\u001b[0m\n",
      "\u001b[38;5;2m✔ Sucessfully compiled vocab\u001b[0m\n",
      "134382 entries, 134209 vectors\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy init-model xx ../models/spacy_grc_model_6_vectors --vectors-loc ../data/word2vec_win2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it into our envoronment\n",
    "nlp = spacy.load(\"../models/spacy_grc_model_6_vectors\")\n",
    "tagger = nlp.create_pipe(\"tagger\")\n",
    "nlp.add_pipe(tagger)\n",
    "optimizer = nlp.begin_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add lookup tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_lemma_lookup = json.load(open(\"../data/ag_lemma_lookup.json\"))\n",
    "ag_lemma_lookup_merged = json.load(open(\"../data/ag_lemma_lookup_merged.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADJ', 'PRON', 'ADP', 'PUNCT', 'DET', 'CCONJ', 'INTJ', 'PART', 'PROPN', 'AUX', 'SCONJ', 'ADV', 'NOUN', 'X', 'NUM', 'VERB']\n"
     ]
    }
   ],
   "source": [
    "print(list(ag_lemma_lookup.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ἐνέργεια'"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag_lemma_lookup[\"NOUN\"][\"ἐνεργειῶν\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_lemma_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, this works but there is a substantial number of cases in which there is a wrong tag, e.g. \"VERB\" - in this case the lemmatization is unsuccessful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ἐνέργεια'"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag_lemma_lookup_merged[\"ἐνεργειῶν\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nlp.vocab.lookups.remove_table(\"lemma_lookup\")\n",
    "    nlp.vocab.lookups.remove_table(\"lemma_lookup_merged\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let add our table to our model\n",
    "table = nlp.vocab.lookups.add_table(\"lemma_lookup\", ag_lemma_lookup)\n",
    "table = nlp.vocab.lookups.add_table(\"lemma_lookup_merged\", ag_lemma_lookup_merged)\n",
    "# nlp.vocab.lookups.remove_table(\"lemma_exc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'μιμνήσκω'"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load it back from the model (to check functionality for future usage)\n",
    "ag_lemma_lookup = nlp.vocab.lookups.get_table(\"lemma_lookup\")\n",
    "ag_lemma_lookup_merged = nlp.vocab.lookups.get_table(\"lemma_lookup_merged\")\n",
    "\n",
    "\n",
    "ag_lemma_lookup[\"VERB\"][\"μνήσομαι\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install greek-accentuation\n",
    "from greek_accentuation.characters import strip_accents\n",
    "from greek_accentuation.syllabify import *\n",
    "from greek_accentuation.accentuation import *\n",
    "\n",
    "\n",
    "\n",
    "def grave_to_acute(string):\n",
    "    GRAVE = \"\\u0300\"\n",
    "    ACUTE = \"\\u0301\"\n",
    "    return unicodedata.normalize(\"NFC\", \"\".join(unicodedata.normalize(\"NFD\", string).replace(GRAVE, ACUTE)))\n",
    "\n",
    "\n",
    "def list_of_possible_accentuations(morph):\n",
    "    try:\n",
    "        if isinstance(morph, str):\n",
    "            morph = strip_accents(morph)\n",
    "            morph = rebreath(morph.lower())\n",
    "            s = syllabify(morph)\n",
    "            morph_vars = []\n",
    "            for accentuation in possible_accentuations(s, default_short=True):\n",
    "                pos, accent = accentuation #add_accentuation(s, accentuation))\n",
    "                final = s[1 - pos:] if pos > 1 else [\"\"]\n",
    "                morph_acc_var = \"\".join(s[:-pos] + [syllable_add_accent(s[-pos], accent)] + final)\n",
    "                morph_vars.append(morph_acc_var)\n",
    "                morph_vars.append(morph_acc_var.capitalize())\n",
    "            return morph_vars\n",
    "        else:\n",
    "            return []\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "def grc_doc_lemmatizer(doc):\n",
    "    for token in doc:\n",
    "        token.lemma_, token.pos_ = lemmatizer(token.text, token.pos_, token.lemma_)\n",
    "    return doc   \n",
    "\n",
    "def apply_nlp(sentences_list):\n",
    "    spacy_docs = []\n",
    "    for sentence in sentences_list:\n",
    "        doc = nlp(sentence)\n",
    "        spacy_docs.append(doc)\n",
    "    return spacy_docs\n",
    "\n",
    "def check_char_validity(token_text, tag):\n",
    "    if re.search(\"\\W\", token_text):\n",
    "        if re.match(\"\\W+$\", token_text):\n",
    "            tag = \"PUNCT\"\n",
    "        else:\n",
    "            if \"’\" not in token_text:\n",
    "                token_text = re.sub(\"\\W\", \"\", token_text)\n",
    "    return token_text, tag\n",
    "\n",
    "# for new tags in specific order...\n",
    "\n",
    "keys = ['PRON', 'ADP', 'PUNCT', 'DET', 'CCONJ', 'INTJ', 'PART', 'ADJ', 'AUX', 'SCONJ', 'ADV', 'NOUN','PROPN', 'VERB','X', 'NUM']\n",
    " \n",
    "def check_other_tags(token_text, tag):\n",
    "    lemma = token_text\n",
    "    match = False\n",
    "    # for new tags in specific order...\n",
    "    for new_tag in keys:\n",
    "        try:\n",
    "            lemma = ag_lemma_lookup[new_tag][token_text]\n",
    "            tag = new_tag\n",
    "            match = True\n",
    "            break\n",
    "        except:\n",
    "            pass\n",
    "    return lemma, tag, match\n",
    "\n",
    "\n",
    "def lemmatizer_v1(token_text, tag, old_lemma=None):\n",
    "    if (old_lemma==None) or (token_text == old_lemma):\n",
    "        lemma = token_text.lower() # start with assigning the word as it stands\n",
    "        try:\n",
    "            lemma = ag_lemma_lookup[tag][token_text]\n",
    "        except:\n",
    "            try:\n",
    "                lemma = ag_lemma_lookup_merged[token_text]\n",
    "            except:\n",
    "                try:\n",
    "                    lemma = ag_lemma_lookup[tag][grave_to_acute(token_text)]\n",
    "                except:\n",
    "                    try:\n",
    "                        lemma = ag_lemma_lookup_merged[grave_to_acute(token_text)]\n",
    "                    except:\n",
    "                        morph_vars = list_of_possible_accentuations(token_text)\n",
    "                        for var in morph_vars:\n",
    "                            try:\n",
    "                                lemma = ag_lemma_lookup[tag][var] \n",
    "                                break\n",
    "                            except:\n",
    "                                try:\n",
    "                                    lemma = ag_lemma_lookup_merged[var]\n",
    "                                    break\n",
    "                                except:\n",
    "                                    pass\n",
    "        return lemma, tag\n",
    "    else:\n",
    "        return old_lemma, tag\n",
    "    \n",
    "def lemmatizer(token_text, tag, old_lemma=None):\n",
    "    if (old_lemma==None) or (token_text == old_lemma):\n",
    "        token_text, tag = check_char_validity(token_text, tag)\n",
    "        lemma = token_text.lower() # start with assigning the word as it stands\n",
    "        try:\n",
    "            lemma = ag_lemma_lookup[tag][token_text]\n",
    "        except:\n",
    "            try:\n",
    "                lemma = ag_lemma_lookup[tag][grave_to_acute(token_text)]\n",
    "            except:\n",
    "                if check_other_tags(token_text, tag)[2] == True:\n",
    "                    lemma, tag, match = check_other_tags(token_text, tag)\n",
    "                else:\n",
    "                        morph_vars = list_of_possible_accentuations(token_text)\n",
    "                        for var in morph_vars:\n",
    "                            try:\n",
    "                                try:\n",
    "                                    lemma = ag_lemma_lookup[tag][var] \n",
    "                                    break\n",
    "                                except:\n",
    "                                    a, b, match = check_other_tags(var, tag)\n",
    "                                    if match ==True:\n",
    "                                        lemma, tag, match = check_other_tags(var, tag)\n",
    "                                        break\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "        return lemma, tag\n",
    "    else:\n",
    "        return old_lemma, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'perseus_tags_correct%': 89.23424440930653, 'perseus_lemmata_correct%': 87.17415857239665}\n"
     ]
    }
   ],
   "source": [
    "# add lemmatizer to the pipeline\n",
    "nlp.remove_pipe(\"grc_doc_lemmatizer\")\n",
    "nlp.add_pipe(grc_doc_lemmatizer, \"grc_doc_lemmatizer\", after=\"tagger\") # grc_doc_lemmatizer\n",
    "\n",
    "model_test = {}\n",
    "perseus_tags_correct = 0\n",
    "perseus_lemmata_correct = 0\n",
    "for sent in perseus_dev:\n",
    "        doc = nlp(sent[0])\n",
    "        predicted_tags = [token.pos_ for token in doc]\n",
    "        predicted_lemmata = [token.lemma_ for token in doc]\n",
    "        #predicted_lemmata = [lemmatizer(token.text, token.pos_) for token in doc]\n",
    "        given_tags = sent[1][\"tags\"]\n",
    "        given_lemmata = sent[1][\"lemmata\"]\n",
    "        perseus_tags_correct += len([p for p, g in zip(predicted_tags, given_tags) if p ==g])\n",
    "        perseus_lemmata_correct += len([p for p, g in zip(predicted_lemmata, given_lemmata) if p ==g])\n",
    "model_test[\"perseus_tags_correct%\"] = (perseus_tags_correct / perseus_dev_len) * 100\n",
    "model_test[\"perseus_lemmata_correct%\"] = (perseus_lemmata_correct / perseus_dev_len) * 100\n",
    "print(model_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ἀπό', 'ADP')"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer('ἀπʼ', 'PUNCT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('δι’', 'ADP')"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer(\"δι’\", \"ADP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Παῦλος', 'NOUN')"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer(\"ΠΑΥΛΟΣ\", \"VERB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('βίβλος', 'NOUN')"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer(\"ΒΙΒΛΟΣ\", \"ADJ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Ἰησοῦς', 'NOUN')"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer(\"Ἰησοῦ\", \"NOUN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('βιββιβλος', 'NOUN')"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nonexistent word\n",
    "lemmatizer(\"ΒΙΒΒΙΒΛΟΣ\", \"NOUN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ἐνέργεια', 'NOUN')"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it works even with wrong POS-tag\n",
    "lemmatizer(\"ἐνεργειῶν\", \"VERB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('θεός', 'NOUN')"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer(\"θεοῦ.\", \"PUNCT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training our own model with gold-parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Ancient_Greek-Perseus/master/grc_perseus-ud-train.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Ancient_Greek-Perseus/master/grc_perseus-ud-dev.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Ancient_Greek-PROIEL/master/grc_proiel-ud-train.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Ancient_Greek-PROIEL/master/grc_proiel-ud-dev.conllu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_encoding(string):\n",
    "    return unicodedata.normalize(\"NFC\", string)\n",
    "v_ud = \"’\"\n",
    "v_agt = \"ʼ\"\n",
    "\n",
    "v_ud == v_agt # because of that, we will everywhere use the agt version, since it does not confuse the word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagged_data_from_url(url):\n",
    "    corpus = pyconll.load.iter_from_url(url)\n",
    "    tagged_data = []\n",
    "    for sentence in corpus:\n",
    "        words, tags, lemmata = [], [], []\n",
    "        for token in sentence:\n",
    "            words.append(normalize_encoding(re.sub(v_ud, v_agt, token.form)))\n",
    "            tags.append(token.upos)\n",
    "            lemmata.append(normalize_encoding(token.lemma))\n",
    "        if \"-dev.\" in url:\n",
    "            tagged_data.append((normalize_encoding(re.sub(v_ud, v_agt, sentence.text)), {\"words\" : words, \"tags\" : tags, \"lemmata\" : lemmata}))\n",
    "        else:\n",
    "            tagged_data.append((normalize_encoding(re.sub(v_ud, v_agt, sentence.text)), {\"words\" : words, \"tags\" : tags}))\n",
    "    print(\"File name: {0}; number of sentences: {1}\".format(url.rpartition(\"/\")[2], len(tagged_data)))\n",
    "    return tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name: grc_perseus-ud-train.conllu; number of sentences: 11476\n",
      "File name: grc_perseus-ud-dev.conllu; number of sentences: 1137\n",
      "File name: grc_proiel-ud-train.conllu; number of sentences: 15014\n",
      "File name: grc_proiel-ud-dev.conllu; number of sentences: 1019\n"
     ]
    }
   ],
   "source": [
    "perseus_train = tagged_data_from_url(urls[0])\n",
    "perseus_dev = tagged_data_from_url(urls[1])\n",
    "proiel_train = tagged_data_from_url(urls[2])\n",
    "proiel_dev = tagged_data_from_url(urls[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ἐρᾷ μὲν ἁγνὸς οὐρανὸς τρῶσαι χθόνα, ἔρως δὲ γαῖαν λαμβάνει γάμου τυχεῖν·', {'words': ['ἐρᾷ', 'μὲν', 'ἁγνὸς', 'οὐρανὸς', 'τρῶσαι', 'χθόνα', ',', 'ἔρως', 'δὲ', 'γαῖαν', 'λαμβάνει', 'γάμου', 'τυχεῖν', '·'], 'tags': ['VERB', 'ADV', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'PUNCT', 'NOUN', 'CCONJ', 'NOUN', 'VERB', 'NOUN', 'VERB', 'PUNCT']})\n"
     ]
    }
   ],
   "source": [
    "print(perseus_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = perseus_train + proiel_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26490"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens in proiel_dev: 13652\n",
      "tokens in perseus_dev: 22135\n"
     ]
    }
   ],
   "source": [
    "proiel_dev_len = sum([len(sent[1][\"tags\"]) for sent in proiel_dev])\n",
    "print(\"tokens in proiel_dev: \" + str(proiel_dev_len))\n",
    "\n",
    "perseus_dev_len = sum([len(sent[1][\"tags\"]) for sent in perseus_dev])\n",
    "print(\"tokens in perseus_dev: \" + str(perseus_dev_len))\n",
    "total_len = proiel_dev_len + perseus_dev_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tests = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add lemmatizer to the pipeline\n",
    "nlp.remove_pipe(\"grc_doc_lemmatizer\")\n",
    "nlp.add_pipe(grc_doc_lemmatizer, \"grc_doc_lemmatizer\", after=\"tagger\") # grc_doc_lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Losses': {'tagger': 19741.383138615638}, 'proiel_tags_correct%': 96.3888075007325, 'proiel_lemmata_correct%': 95.54644008203927, 'perseus_tags_correct%': 89.56403885249604, 'perseus_lemmata_correct%': 87.23740682177547}\n",
      "{'Losses': {'tagger': 19596.767333576223}, 'proiel_tags_correct%': 96.3961324348081, 'proiel_lemmata_correct%': 95.53179021388807, 'perseus_tags_correct%': 89.3426699796702, 'perseus_lemmata_correct%': 87.15608764400271}\n"
     ]
    }
   ],
   "source": [
    "n_iter = 10\n",
    "\n",
    "model_tests = []\n",
    "for i in range(n_iter):\n",
    "    \n",
    "    ### TRAIN THE MODEL\n",
    "    random.shuffle(train_data)\n",
    "    losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(texts, annotations, drop=0.3, sgd=optimizer, losses=losses)\n",
    "    \n",
    "    ### TEST THE MODEL\n",
    "    model_test = {}\n",
    "    model_test[\"Losses\"] = losses\n",
    "    proiel_tags_correct = 0\n",
    "    proiel_lemmata_correct = 0\n",
    "    for sent in proiel_dev:\n",
    "        doc = nlp(sent[0])\n",
    "        predicted_tags = [token.pos_ for token in doc]\n",
    "        predicted_lemmata = [token.lemma_ for token in doc]\n",
    "        #predicted_lemmata = [lemmatizer(token.text, token.pos_) for token in doc]\n",
    "        given_tags = sent[1][\"tags\"]\n",
    "        given_lemmata = sent[1][\"lemmata\"]\n",
    "        proiel_tags_correct += len([p for p, g in zip(predicted_tags, given_tags) if p ==g])\n",
    "        proiel_lemmata_correct += len([p for p, g in zip(predicted_lemmata, given_lemmata) if p ==g])\n",
    "    model_test[\"proiel_tags_correct%\"] = (proiel_tags_correct / proiel_dev_len) * 100\n",
    "    model_test[\"proiel_lemmata_correct%\"] = (proiel_lemmata_correct / proiel_dev_len) * 100\n",
    "\n",
    "    perseus_tags_correct = 0\n",
    "    perseus_lemmata_correct = 0\n",
    "    for sent in perseus_dev:\n",
    "        doc = nlp(sent[0])\n",
    "        predicted_tags = [token.pos_ for token in doc]\n",
    "        predicted_lemmata = [token.lemma_ for token in doc]\n",
    "        #predicted_lemmata = [lemmatizer(token.text, token.pos_) for token in doc]\n",
    "        given_tags = sent[1][\"tags\"]\n",
    "        given_lemmata = sent[1][\"lemmata\"]\n",
    "        perseus_tags_correct += len([p for p, g in zip(predicted_tags, given_tags) if p ==g])\n",
    "        perseus_lemmata_correct += len([p for p, g in zip(predicted_lemmata, given_lemmata) if p ==g])\n",
    "    model_test[\"perseus_tags_correct%\"] = (perseus_tags_correct / perseus_dev_len) * 100\n",
    "    model_test[\"perseus_lemmata_correct%\"] = (perseus_lemmata_correct / perseus_dev_len) * 100\n",
    "    print(model_test)\n",
    "    model_tests.append(model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.remove_pipe(\"grc_doc_lemmatizer\")\n",
    "nlp.to_disk(\"../models/spacy_grc_model_6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'proiel_tags_correct%': 93.59068268385585,\n",
       " 'proiel_lemmata_correct%': 91.31995312042191,\n",
       " 'perseus_tags_correct%': 85.05534221820646,\n",
       " 'perseus_lemmata_correct%': 89.01739326857917}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMPARE WITH OUR FIRST MODEL\n",
    "nlp_1 = spacy.load(\"../models/spacy_grc_model_1\")\n",
    "\n",
    "model_test = {}\n",
    "proiel_tags_correct = 0\n",
    "proiel_lemmata_correct = 0\n",
    "for sent in proiel_dev:\n",
    "    doc = nlp_1(sent[0])\n",
    "    predicted_tags = [token.pos_ for token in doc]\n",
    "    predicted_lemmata = [lemmatizer(token.text, token.pos_) for token in doc]\n",
    "    given_tags = sent[1][\"tags\"]\n",
    "    given_lemmata = sent[1][\"lemmata\"]\n",
    "    proiel_tags_correct += len([p for p, g in zip(predicted_tags, given_tags) if p ==g])\n",
    "    proiel_lemmata_correct += len([p for p, g in zip(predicted_lemmata, given_lemmata) if p ==g])\n",
    "model_test[\"proiel_tags_correct%\"] = (proiel_tags_correct / proiel_dev_len) * 100\n",
    "model_test[\"proiel_lemmata_correct%\"] = (proiel_lemmata_correct / proiel_dev_len) * 100\n",
    "\n",
    "perseus_tags_correct = 0\n",
    "perseus_lemmata_correct = 0\n",
    "for sent in perseus_dev:\n",
    "    doc = nlp_1(sent[0])\n",
    "    predicted_tags = [token.pos_ for token in doc]\n",
    "    predicted_lemmata = [lemmatizer(token.text, token.pos_) for token in doc]\n",
    "    given_tags = sent[1][\"tags\"]\n",
    "    given_lemmata = sent[1][\"lemmata\"]\n",
    "    perseus_tags_correct += len([p for p, g in zip(predicted_tags, given_tags) if p ==g])\n",
    "    perseus_lemmata_correct += len([p for p, g in zip(predicted_lemmata, given_lemmata) if p ==g])\n",
    "model_test[\"perseus_tags_correct%\"] = (perseus_tags_correct / perseus_dev_len) * 100\n",
    "model_test[\"perseus_lemmata_correct%\"] = (perseus_lemmata_correct / perseus_dev_len) * 100\n",
    "\n",
    "model_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our results with first model:\n",
    "```\n",
    "{'proiel_tags_correct%': 87.90653384119544,\n",
    " 'proiel_lemmata_correct%': 85.62115440961031,\n",
    " 'perseus_tags_correct%': 85.05534221820646,\n",
    " 'perseus_lemmata_correct%': 82.66997967020556}\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(\"../models/spacy_grc_model_6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGT = sddk.read_file(\"SDAM_data/AGT/AGT_preprocessed_20201127.json\", \"df\", conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"../models/spacy_grc_model_6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(grc_doc_lemmatizer, \"grc_doc_lemmatizer\", after=\"tagger\") # grc_doc_lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ΠΑΥΛΟΣ', 'VERB', 'Παῦλος'), ('ἀπόστολος', 'NOUN', 'ἀπόστολος'), (',', 'PUNCT', ','), ('οὐκ', 'ADV', 'οὐ'), ('ἀπʼ', 'ADP', 'Ἆπις'), ('ἀνθρώπων', 'NOUN', 'ἄνθρωπος'), ('οὐδὲ', 'CCONJ', 'οὐδέ'), ('διʼ', 'ADP', 'Ζεύς'), ('ἀνθρώπου', 'NOUN', 'ἄνθρωπος'), ('ἀλλὰ', 'CCONJ', 'ἀλλά'), ('διὰ', 'ADP', 'διά'), ('Ἰησοῦ', 'PROPN', 'Ἰησοῦς'), ('Χριστοῦ', 'PROPN', 'Χριστός'), ('καὶ', 'CCONJ', 'καί'), ('θεοῦ', 'NOUN', 'θεός'), ('πατρὸς', 'NOUN', 'πατήρ'), ('τοῦ', 'DET', 'ὁ'), ('ἐγείραντος', 'VERB', 'ἐγείρω'), ('αὐτὸν', 'PRON', 'αὐτός'), ('ἐκ', 'ADP', 'ἐκ'), ('νεκρῶν', 'NOUN', 'νεκρός'), (',', 'PUNCT', ','), ('καὶ', 'CCONJ', 'καί'), ('οἱ', 'DET', 'ὁ'), ('σὺν', 'ADP', 'σύν'), ('ἐμοὶ', 'PRON', 'ἐγώ'), ('πάντες', 'ADJ', 'πᾶς'), ('ἀδελφοί', 'NOUN', 'ἀδελφός'), (',', 'PUNCT', ','), ('ταῖς', 'DET', 'ὁ'), ('ἐκκλησίαις', 'NOUN', 'ἐκκλησία'), ('τῆς', 'DET', 'ὁ'), ('Γαλατίας', 'NOUN', 'Γαλατία'), ('·', 'PUNCT', '·')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(AGT[AGT[\"author_id\"]==\"tlg0031paul\"].iloc[3][\"sentences\"][0])\n",
    "print([(token.text, token.pos_, token.lemma_) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ΠΑΥΛΟΣ', 'VERB', 'Παῦλος'), ('ἀπόστολος', 'NOUN', 'ἀπόστολος'), (',', 'PUNCT', ','), ('οὐκ', 'ADV', 'οὐ'), ('ἀπʼ', 'ADP', 'ἀπό'), ('ἀνθρώπων', 'NOUN', 'ἄνθρωπος'), ('οὐδὲ', 'CCONJ', 'οὐδέ'), ('διʼ', 'ADP', 'διά'), ('ἀνθρώπου', 'NOUN', 'ἄνθρωπος'), ('ἀλλὰ', 'CCONJ', 'ἀλλά'), ('διὰ', 'ADP', 'διά'), ('Ἰησοῦ', 'PROPN', 'Ἰησοῦς'), ('Χριστοῦ', 'PROPN', 'Χριστός'), ('καὶ', 'CCONJ', 'καί'), ('θεοῦ', 'NOUN', 'θεός'), ('πατρὸς', 'NOUN', 'πατήρ'), ('τοῦ', 'DET', 'ὁ'), ('ἐγείραντος', 'VERB', 'ἐγείρω'), ('αὐτὸν', 'PRON', 'αὐτός'), ('ἐκ', 'ADP', 'ἐκ'), ('νεκρῶν', 'NOUN', 'νεκρών'), (',', 'PUNCT', ','), ('καὶ', 'CCONJ', 'καί'), ('οἱ', 'DET', 'ὁ'), ('σὺν', 'ADP', 'σύν'), ('ἐμοὶ', 'PRON', 'ἐγώ'), ('πάντες', 'ADJ', 'πᾶς'), ('ἀδελφοί', 'NOUN', 'ἀδελφός'), (',', 'PUNCT', ','), ('ταῖς', 'DET', 'ὁ'), ('ἐκκλησίαις', 'NOUN', 'ἐκκλησία'), ('τῆς', 'DET', 'ὁ'), ('Γαλατίας', 'NOUN', 'Γαλατία'), ('·', 'PUNCT', '·')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(AGT[AGT[\"author_id\"]==\"tlg0031paul\"].iloc[3][\"sentences\"][0])\n",
    "print([(token.text, token.pos_, token.lemma_) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('grc_doc_lemmatizer', <function __main__.grc_doc_lemmatizer(doc)>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.remove_pipe(\"grc_doc_lemmatizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(\"../models/spacy_grc_model_6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's continue with testing our old functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('πᾶσα', 'DET', 'πᾶς'), ('τέχνη', 'NOUN', 'τέχνη'), ('καὶ', 'CCONJ', 'καί'), ('πᾶσα', 'DET', 'πᾶς'), ('μέθοδος', 'NOUN', 'μέθοδος'), (',', 'PUNCT', ','), ('ὁμοίως', 'ADV', 'ὁμοίως'), ('δὲ', 'PART', 'δέ'), ('πρᾶξίς', 'ADJ', 'πρᾶξις'), ('τε', 'PART', 'τε'), ('καὶ', 'CCONJ', 'καί'), ('προαίρεσις', 'NOUN', 'προαίρεσις'), (',', 'PUNCT', ','), ('ἀγαθοῦ', 'ADJ', 'ἀγαθός'), ('τινὸς', 'PRON', 'τίς'), ('ἐφίεσθαι', 'VERB', 'ἐφίημι'), ('δοκεῖ', 'VERB', 'δοκέω'), ('·', 'PUNCT', '·')]\n",
      "[('διὸ', 'ADV', 'διό'), ('καλῶς', 'ADV', 'καλῶς'), ('ἀπεφήναντο', 'VERB', 'ἀποφαίνω'), ('τἀγαθόν', 'ADJ', 'ἀγαθός'), (',', 'PUNCT', ','), ('οὗ', 'PRON', 'ὅς'), ('πάντʼ', 'ADJ', 'πᾶς'), ('ἐφίεται', 'VERB', 'ἐφίημι'), ('.', 'PUNCT', '.')]\n",
      "[('διαφορὰ', 'NOUN', 'διαφορά'), ('δέ', 'ADV', 'δέ'), ('τις', 'PRON', 'τίς'), ('φαίνεται', 'VERB', 'φαίνω'), ('τῶν', 'DET', 'ὁ'), ('τελῶν', 'VERB', 'τελέω'), ('·', 'PUNCT', '·')]\n",
      "[('τὰ', 'DET', 'ὁ'), ('μὲν', 'PART', 'μέν'), ('γάρ', 'PART', 'γάρ'), ('εἰσιν', 'VERB', 'εἰμί'), ('ἐνέργειαι', 'ADJ', 'ἐνέργεια'), (',', 'PUNCT', ','), ('τὰ', 'DET', 'ὁ'), ('δὲ', 'CCONJ', 'δέ'), ('παρʼ', 'ADP', 'Πάρος'), ('αὐτὰς', 'ADJ', 'αὐτός'), ('ἔργα', 'NOUN', 'ἔργον'), ('τινά', 'PRON', 'τις'), ('.', 'PUNCT', '.')]\n",
      "[('ὧν', 'PRON', 'ὅς'), ('δʼ', 'ADV', 'δέ'), ('εἰσὶ', 'AUX', 'εἰμί'), ('τέλη', 'NOUN', 'τέλος'), ('τινὰ', 'PRON', 'τίς'), ('παρὰ', 'ADP', 'παρά'), ('τὰς', 'DET', 'ὁ'), ('πράξεις', 'NOUN', 'πρᾶξις'), (',', 'PUNCT', ','), ('ἐν', 'ADP', 'ἐν'), ('τούτοις', 'PRON', 'οὗτος'), ('βελτίω', 'ADJ', 'βελτίων'), ('πέφυκε', 'VERB', 'φύω'), ('τῶν', 'DET', 'ὁ'), ('ἐνεργειῶν', 'ADJ', 'ἐνέργεια'), ('τὰ', 'DET', 'ὁ'), ('ἔργα', 'NOUN', 'ἔργον'), ('.', 'PUNCT', '.')]\n",
      "[('πολλῶν', 'ADJ', 'πολύς'), ('δὲ', 'ADV', 'δέ'), ('πράξεων', 'NOUN', 'πρᾶξις'), ('οὐσῶν', 'AUX', 'εἰμί'), ('καὶ', 'CCONJ', 'καί'), ('τεχνῶν', 'ADJ', 'τέχνη'), ('καὶ', 'CCONJ', 'καί'), ('ἐπιστημῶν', 'NOUN', 'ἐπιστήμη'), ('πολλὰ', 'ADJ', 'πολύς'), ('γίνεται', 'VERB', 'γίγνομαι'), ('καὶ', 'CCONJ', 'καί'), ('τὰ', 'DET', 'ὁ'), ('τέλη', 'NOUN', 'τέλος'), ('·', 'PUNCT', '·')]\n",
      "[('ἰατρικῆς', 'NOUN', 'ἰατρικός'), ('μὲν', 'PART', 'μέν'), ('γὰρ', 'PART', 'γάρ'), ('ὑγίεια', 'NOUN', 'ὑγίεια'), (',', 'PUNCT', ','), ('ναυπηγικῆς', 'ADJ', 'ναυπηγικός'), ('δὲ', 'CCONJ', 'δέ'), ('πλοῖον', 'NOUN', 'πλοῖον'), (',', 'PUNCT', ','), ('στρατηγικῆς', 'ADJ', 'στρατηγικός'), ('δὲ', 'CCONJ', 'δέ'), ('νίκη', 'NOUN', 'νίκη'), (',', 'PUNCT', ','), ('οἰκονομικῆς', 'ADJ', 'οἰκονομικός'), ('δὲ', 'PART', 'δέ'), ('πλοῦτος', 'NOUN', 'πλοῦτος'), ('.', 'PUNCT', '.')]\n",
      "[('ὅσαι', 'PRON', 'ὅσος'), ('δʼ', 'ADV', 'δέ'), ('εἰσὶ', 'AUX', 'εἰμί'), ('τῶν', 'DET', 'ὁ'), ('τοιούτων', 'ADJ', 'τοιοῦτος'), ('ὑπὸ', 'ADP', 'ὑπό'), ('μίαν', 'ADJ', 'εἷς'), ('τινὰ', 'PRON', 'τίς'), ('δύναμιν', 'NOUN', 'δύναμις'), (',', 'PUNCT', ','), ('καθάπερ', 'SCONJ', 'καθά'), ('ὑπὸ', 'ADP', 'ὑπό'), ('τὴν', 'DET', 'ὁ'), ('ἱππικὴν', 'ADJ', 'ἱππικός'), ('χαλινοποιικὴ', 'ADJ', 'χαλινοποιική'), ('καὶ', 'CCONJ', 'καί'), ('ὅσαι', 'PRON', 'ὅσος'), ('ἄλλαι', 'DET', 'ἄλλος'), ('τῶν', 'DET', 'ὁ'), ('ἱππικῶν', 'ADJ', 'ἱππικός'), ('ὀργάνων', 'NOUN', 'ὄργανον'), ('εἰσίν', 'VERB', 'εἰμί'), (',', 'PUNCT', ','), ('αὕτη', 'PRON', 'αὐτός'), ('δὲ', 'CCONJ', 'δέ'), ('καὶ', 'ADV', 'καί'), ('πᾶσα', 'DET', 'πᾶς'), ('πολεμικὴ', 'ADJ', 'πολεμικός'), ('πρᾶξις', 'NOUN', 'πρᾶξις'), ('ὑπὸ', 'ADP', 'ὑπό'), ('τὴν', 'DET', 'ὁ'), ('στρατηγικήν', 'ADJ', 'στρατηγικός'), (',', 'PUNCT', ','), ('κατὰ', 'ADP', 'κατά'), ('τὸν', 'DET', 'ὁ'), ('αὐτὸν', 'ADJ', 'αὐτός'), ('δὴ', 'ADV', 'δή'), ('τρόπον', 'NOUN', 'τρόπος'), ('ἄλλαι', 'ADJ', 'ἄλλος'), ('ὑφʼ', 'ADP', 'ὑφή'), ('ἑτέρας', 'ADJ', 'ἕτερος'), ('·', 'PUNCT', '·')]\n",
      "[('ἐν', 'ADP', 'ἐν'), ('ἁπάσαις', 'ADJ', 'ἅπας'), ('δὲ', 'ADV', 'δέ'), ('τὰ', 'DET', 'ὁ'), ('τῶν', 'DET', 'ὁ'), ('ἀρχιτεκτονικῶν', 'VERB', 'ἀρχιτεκτονικῶν'), ('τέλη', 'NOUN', 'τέλος'), ('πάντων', 'DET', 'πᾶς'), ('ἐστὶν', 'AUX', 'εἰμί'), ('αἱρετώτερα', 'ADJ', 'αἱρετός'), ('τῶν', 'DET', 'ὁ'), ('ὑπʼ', 'ADP', 'ὑπό'), ('αὐτά', 'PRON', 'αὐτός'), ('·', 'PUNCT', '·')]\n",
      "[('τούτων', 'ADJ', 'οὗτος'), ('γὰρ', 'PART', 'γάρ'), ('χάριν', 'NOUN', 'χάρις'), ('κἀκεῖνα', 'ADJ', 'ἐκεῖνος'), ('διώκεται', 'VERB', 'διώκω'), ('.', 'PUNCT', '.')]\n"
     ]
    }
   ],
   "source": [
    "arist_test = apply_nlp(AGT[AGT[\"doc_id\"]==\"tlg0086.tlg010\"][\"sentences\"].tolist()[0])\n",
    "for doc in arist_test[:10]:\n",
    "    print([(token.text, token.pos_, token.lemma_) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
