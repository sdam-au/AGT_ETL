{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pyconll\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang=\"el\"\n",
    "# initatize en empty model\n",
    "nlp = spacy.blank(lang)\n",
    "# add the tagger to the pipeline\n",
    "# nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "tagger = nlp.create_pipe(\"tagger\")\n",
    "    # Add the tags. This needs to be done before you start training.\n",
    "#for tag, values in TAG_MAP.items():\n",
    "#    tagger.add_label(tag, values)\n",
    "nlp.add_pipe(tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/ag_lemma_lookup.json\"\n",
    "ag_lemma_lookup = json.load(open(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "lemmatizer = Lemmatizer(lookups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training our own model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract first part of training data\n",
    "corpus_perseus = pyconll.load.iter_from_url(\"https://raw.githubusercontent.com/UniversalDependencies/UD_Ancient_Greek-Perseus/master/grc_perseus-ud-train.conllu\")\n",
    "\n",
    "corpus_proiel = pyconll.load.iter_from_url(\"https://raw.githubusercontent.com/UniversalDependencies/UD_Ancient_Greek-PROIEL/master/grc_proiel-ud-train.conllu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_perseus = []\n",
    "for sentence in corpus_perseus:\n",
    "    words, tags = [], []\n",
    "    for token in sentence:\n",
    "        words.append(token.form)\n",
    "        tags.append(token.upos)\n",
    "    train_data_perseus.append((sentence.text, {\"words\" : words, \"tags\" : tags}))\n",
    "        #forms_lemmas_dict[token.form] = [{\"l\" : token.lemma, \"p\" : token.xpos, \"s\" : \"\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_proiel = []\n",
    "for sentence in corpus_proiel:\n",
    "    words, tags = [], []\n",
    "    for token in sentence:\n",
    "        words.append(token.form)\n",
    "        tags.append(token.upos)\n",
    "    train_data_proiel.append((sentence.text, {\"words\" : words, \"tags\" : tags}))\n",
    "        #forms_lemmas_dict[token.form] = [{\"l\" : token.lemma, \"p\" : token.xpos, \"s\" : \"\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_proiel[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data_perseus + train_data_proiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang=\"xx\"\n",
    "nlp = spacy.blank(lang)\n",
    "tagger = nlp.create_pipe(\"tagger\")\n",
    "\n",
    "nlp.add_pipe(tagger)\n",
    "\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "n_iter = 10\n",
    "for i in range(n_iter):\n",
    "    random.shuffle(train_data)\n",
    "    losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(texts, annotations, sgd=optimizer, losses=losses)\n",
    "    print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"ἀρχόμενος σέο, Φοῖβε, παλαιγενέων κλέα φωτῶν μνήσομαι, οἳ Πόντοιο κατὰ στόμα καὶ διὰ πέτρας Κυανέας βασιλῆος ἐφημοσύνῃ Πελίαο χρύσειον μετὰ κῶας ἐύζυγον ἤλασαν Ἀργώ.\"\n",
    "doc = nlp(test_text)\n",
    "print(\"Tags\", [(t.text, t.tag_, t.pos_) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"ἀρχόμενος σέο, Φοῖβε, παλαιγενέων κλέα φωτῶν μνήσομαι, οἳ Πόντοιο κατὰ στόμα καὶ διὰ πέτρας Κυανέας βασιλῆος ἐφημοσύνῃ Πελίαο χρύσειον μετὰ κῶας ἐύζυγον ἤλασαν Ἀργώ.\"\n",
    "doc = nlp(test_text)\n",
    "print(\"Tags\", [(t.text, t.tag_, t.pos_) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(\"../data/spacy_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it back\n",
    "nlp = spacy.load(\"../spacy_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab.lookups.tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab.lookups.remove_table(\"ag_lemma_lookup\")\n",
    "nlp.vocab.lookups.remove_table(\"lemma_exc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/ag_lemma_lookup.json\"\n",
    "lemma_lookup = json.load(open(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let add our table to our model\n",
    "#table = nlp.vocab.lookups.add_table(\"ag_lemma_lookup\", ag_lemma_lookup)\n",
    "table = nlp.vocab.lookups.add_table(\"lemma_lookup\", lemma_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(\"../spacy_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'μιμνήσκω'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_lookup = nlp.vocab.lookups.get_table(\"lemma_lookup\")\n",
    "lemma_lookup[\"VERB\"][\"μνήσομαι\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
