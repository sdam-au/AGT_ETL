{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyconll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"xx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"This is my sentence I want to play with.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This', '')\n('is', '')\n('my', '')\n('sentence', '')\n('I', '')\n('want', '')\n('to', '')\n('play', '')\n('with', '')\n('.', '')\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print((token.text, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training our own model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple test with few English sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = [\n",
    "    (\"I like green eggs\", {\"words\" : [\"I\", \"like\", \"green\", \"eggs\"], \"tags\": [\"N\", \"V\", \"J\", \"N\"]}),\n",
    "    (\"Eat blue ham\", {\"words\": ['Eat', 'blue', 'ham'], \"tags\": [\"V\", \"J\", \"N\"]}),]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_MAP = {\"N\": {\"pos\": \"NOUN\"}, \"V\": {\"pos\": \"VERB\"}, \"J\": {\"pos\": \"ADJ\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'tagger': 6.681818962097168}\nLosses {'tagger': 6.5615010261535645}\nLosses {'tagger': 6.222362995147705}\nLosses {'tagger': 5.625188827514648}\nLosses {'tagger': 4.727659225463867}\nLosses {'tagger': 3.612746238708496}\nLosses {'tagger': 2.466280221939087}\nLosses {'tagger': 1.4634302854537964}\nLosses {'tagger': 0.7310025691986084}\nLosses {'tagger': 0.3093547523021698}\n"
     ]
    }
   ],
   "source": [
    "lang=\"xx\"\n",
    "nlp = spacy.blank(lang)\n",
    "# add the tagger to the pipeline\n",
    "# nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "tagger = nlp.create_pipe(\"tagger\")\n",
    "    # Add the tags. This needs to be done before you start training.\n",
    "for tag, values in TAG_MAP.items():\n",
    "    tagger.add_label(tag, values)\n",
    "nlp.add_pipe(tagger)\n",
    "\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "\n",
    "n_iter = 10\n",
    "for i in range(n_iter):\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(texts, annotations, sgd=optimizer, losses=losses)\n",
    "    print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags [('I', 'N', 'NOUN'), ('like', 'V', 'VERB'), ('blue', 'J', 'ADJ'), ('eggs', 'N', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "test_text = \"I like blue eggs\"\n",
    "doc = nlp(test_text)\n",
    "print(\"Tags\", [(t.text, t.tag_, t.pos_) for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greek test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract first part of training data\n",
    "corpus_perseus = pyconll.load.iter_from_url(\"https://raw.githubusercontent.com/UniversalDependencies/UD_Ancient_Greek-Perseus/master/grc_perseus-ud-train.conllu\")\n",
    "\n",
    "corpus_proiel = pyconll.load.iter_from_url(\"https://raw.githubusercontent.com/UniversalDependencies/UD_Ancient_Greek-PROIEL/master/grc_proiel-ud-train.conllu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_perseus = []\n",
    "for sentence in corpus_perseus:\n",
    "    words, tags = [], []\n",
    "    for token in sentence:\n",
    "        words.append(token.form)\n",
    "        tags.append(token.upos)\n",
    "    train_data_perseus.append((sentence.text, {\"words\" : words, \"tags\" : tags}))\n",
    "        #forms_lemmas_dict[token.form] = [{\"l\" : token.lemma, \"p\" : token.xpos, \"s\" : \"\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ἐρᾷ μὲν ἁγνὸς οὐρανὸς τρῶσαι χθόνα, ἔρως δὲ γαῖαν λαμβάνει γάμου τυχεῖν·', {'words': ['ἐρᾷ', 'μὲν', 'ἁγνὸς', 'οὐρανὸς', 'τρῶσαι', 'χθόνα', ',', 'ἔρως', 'δὲ', 'γαῖαν', 'λαμβάνει', 'γάμου', 'τυχεῖν', '·'], 'tags': ['VERB', 'ADV', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'PUNCT', 'NOUN', 'CCONJ', 'NOUN', 'VERB', 'NOUN', 'VERB', 'PUNCT']}), ('ὄμβρος δ̓ ἀπ̓ εὐνάοντος οὐρανοῦ πεσὼν ἔκυσε γαῖαν·', {'words': ['ὄμβρος', 'δ̓', 'ἀπ̓', 'εὐνάοντος', 'οὐρανοῦ', 'πεσὼν', 'ἔκυσε', 'γαῖαν', '·'], 'tags': ['NOUN', 'ADV', 'ADP', 'ADJ', 'NOUN', 'VERB', 'VERB', 'NOUN', 'PUNCT']}), ('ἡ δὲ τίκτεται βροτοῖς μήλων τε βοσκὰς καὶ βίον Δημήτριον·', {'words': ['ἡ', 'δὲ', 'τίκτεται', 'βροτοῖς', 'μήλων', 'τε', 'βοσκὰς', 'καὶ', 'βίον', 'Δημήτριον', '·'], 'tags': ['DET', 'ADV', 'VERB', 'NOUN', 'NOUN', 'ADV', 'NOUN', 'CCONJ', 'NOUN', 'ADJ', 'PUNCT']})]\n"
     ]
    }
   ],
   "source": [
    "print(train_data_perseus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_proiel = []\n",
    "for sentence in corpus_proiel:\n",
    "    words, tags = [], []\n",
    "    for token in sentence:\n",
    "        words.append(token.form)\n",
    "        tags.append(token.upos)\n",
    "    train_data_proiel.append((sentence.text, {\"words\" : words, \"tags\" : tags}))\n",
    "        #forms_lemmas_dict[token.form] = [{\"l\" : token.lemma, \"p\" : token.xpos, \"s\" : \"\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Περσέων μέν νυν οἱ λόγιοι Φοίνικας αἰτίους φασὶ γενέσθαι τῆς διαφορῆς', {'words': ['Περσέων', 'μέν', 'νυν', 'οἱ', 'λόγιοι', 'Φοίνικας', 'αἰτίους', 'φασὶ', 'γενέσθαι', 'τῆς', 'διαφορῆς'], 'tags': ['NOUN', 'ADV', 'ADV', 'DET', 'NOUN', 'NOUN', 'ADJ', 'VERB', 'VERB', 'DET', 'NOUN']}), ('τούτους γὰρ ἀπὸ τῆς Ἐρυθρῆς καλεομένης θαλάσσης ἀπικομένους ἐπὶ τήνδε τὴν θάλασσαν καὶ οἰκήσαντας τοῦτον τὸν χῶρον τὸν καὶ νῦν οἰκέουσι αὐτίκα ναυτιλίῃσι μακρῇσι ἐπιθέσθαι ἀπαγινέοντας δὲ φορτία Αἰγύπτιά τε καὶ Ἀσσύρια τῇ τε ἄλλῃ ἐσαπικνέεσθαι καὶ δὴ καὶ ἐς Ἄργος', {'words': ['τούτους', 'γὰρ', 'ἀπὸ', 'τῆς', 'Ἐρυθρῆς', 'καλεομένης', 'θαλάσσης', 'ἀπικομένους', 'ἐπὶ', 'τήνδε', 'τὴν', 'θάλασσαν', 'καὶ', 'οἰκήσαντας', 'τοῦτον', 'τὸν', 'χῶρον', 'τὸν', 'καὶ', 'νῦν', 'οἰκέουσι', 'αὐτίκα', 'ναυτιλίῃσι', 'μακρῇσι', 'ἐπιθέσθαι', 'ἀπαγινέοντας', 'δὲ', 'φορτία', 'Αἰγύπτιά', 'τε', 'καὶ', 'Ἀσσύρια', 'τῇ', 'τε', 'ἄλλῃ', 'ἐσαπικνέεσθαι', 'καὶ', 'δὴ', 'καὶ', 'ἐς', 'Ἄργος'], 'tags': ['ADJ', 'ADV', 'ADP', 'DET', 'ADJ', 'VERB', 'NOUN', 'VERB', 'ADP', 'DET', 'DET', 'NOUN', 'CCONJ', 'VERB', 'DET', 'DET', 'NOUN', 'PRON', 'CCONJ', 'ADV', 'VERB', 'ADV', 'NOUN', 'ADJ', 'VERB', 'VERB', 'ADV', 'NOUN', 'ADJ', 'CCONJ', 'CCONJ', 'ADJ', 'DET', 'CCONJ', 'ADV', 'VERB', 'CCONJ', 'ADV', 'ADV', 'ADP', 'PROPN']}), ('τὸ δὲ Ἄργος τοῦτον τὸν χρόνον προεῖχε ἅπασι τῶν ἐν τῇ νῦν Ἑλλάδι καλεομένῃ χωρῇ', {'words': ['τὸ', 'δὲ', 'Ἄργος', 'τοῦτον', 'τὸν', 'χρόνον', 'προεῖχε', 'ἅπασι', 'τῶν', 'ἐν', 'τῇ', 'νῦν', 'Ἑλλάδι', 'καλεομένῃ', 'χωρῇ'], 'tags': ['DET', 'ADV', 'PROPN', 'DET', 'DET', 'NOUN', 'VERB', 'ADJ', 'DET', 'ADP', 'DET', 'ADV', 'PROPN', 'VERB', 'NOUN']})]\n"
     ]
    }
   ],
   "source": [
    "print(train_data_proiel[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data_perseus + train_data_proiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'tagger': 67534.63089752197}\n",
      "Losses {'tagger': 40686.78945232928}\n",
      "Losses {'tagger': 30711.836481655948}\n",
      "Losses {'tagger': 24673.386923532}\n",
      "Losses {'tagger': 20724.32412486011}\n",
      "Losses {'tagger': 17717.56746959647}\n",
      "Losses {'tagger': 15355.740474200109}\n",
      "Losses {'tagger': 13496.945424583624}\n",
      "Losses {'tagger': 12036.107190164388}\n",
      "Losses {'tagger': 10968.727297964346}\n"
     ]
    }
   ],
   "source": [
    "lang=\"xx\"\n",
    "nlp = spacy.blank(lang)\n",
    "# add the tagger to the pipeline\n",
    "# nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "tagger = nlp.create_pipe(\"tagger\")\n",
    "    # Add the tags. This needs to be done before you start training.\n",
    "#for tag, values in TAG_MAP.items():\n",
    "#    tagger.add_label(tag, values)\n",
    "nlp.add_pipe(tagger)\n",
    "\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "n_iter = 10\n",
    "for i in range(n_iter):\n",
    "    random.shuffle(train_data)\n",
    "    losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(texts, annotations, sgd=optimizer, losses=losses)\n",
    "    print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags [('ἀρχόμενος', 'VERB', 'VERB'), ('σέο', 'PRON', 'PRON'), (',', 'PUNCT', 'PUNCT'), ('Φοῖβε', 'NOUN', 'NOUN'), (',', 'PUNCT', 'PUNCT'), ('παλαιγενέων', 'ADJ', 'ADJ'), ('κλέα', 'NOUN', 'NOUN'), ('φωτῶν', 'NOUN', 'NOUN'), ('μνήσομαι', 'VERB', 'VERB'), (',', 'PUNCT', 'PUNCT'), ('οἳ', 'PRON', 'PRON'), ('Πόντοιο', 'NOUN', 'NOUN'), ('κατὰ', 'ADP', 'ADP'), ('στόμα', 'NOUN', 'NOUN'), ('καὶ', 'CCONJ', 'CCONJ'), ('διὰ', 'ADP', 'ADP'), ('πέτρας', 'NOUN', 'NOUN'), ('Κυανέας', 'NOUN', 'NOUN'), ('βασιλῆος', 'NOUN', 'NOUN'), ('ἐφημοσύνῃ', 'VERB', 'VERB'), ('Πελίαο', 'NOUN', 'NOUN'), ('χρύσειον', 'ADJ', 'ADJ'), ('μετὰ', 'ADP', 'ADP'), ('κῶας', 'NOUN', 'NOUN'), ('ἐύζυγον', 'VERB', 'VERB'), ('ἤλασαν', 'VERB', 'VERB'), ('Ἀργώ', 'ADV', 'ADV'), ('.', 'PUNCT', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "test_text = \"ἀρχόμενος σέο, Φοῖβε, παλαιγενέων κλέα φωτῶν μνήσομαι, οἳ Πόντοιο κατὰ στόμα καὶ διὰ πέτρας Κυανέας βασιλῆος ἐφημοσύνῃ Πελίαο χρύσειον μετὰ κῶας ἐύζυγον ἤλασαν Ἀργώ.\"\n",
    "doc = nlp(test_text)\n",
    "print(\"Tags\", [(t.text, t.tag_, t.pos_) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags [('ἀρχόμενος', 'VERB', 'VERB'), ('σέο', 'PRON', 'PRON'), (',', 'PUNCT', 'PUNCT'), ('Φοῖβε', 'NOUN', 'NOUN'), (',', 'PUNCT', 'PUNCT'), ('παλαιγενέων', 'VERB', 'VERB'), ('κλέα', 'NOUN', 'NOUN'), ('φωτῶν', 'NOUN', 'NOUN'), ('μνήσομαι', 'VERB', 'VERB'), (',', 'PUNCT', 'PUNCT'), ('οἳ', 'PRON', 'PRON'), ('Πόντοιο', 'NOUN', 'NOUN'), ('κατὰ', 'ADP', 'ADP'), ('στόμα', 'NOUN', 'NOUN'), ('καὶ', 'CCONJ', 'CCONJ'), ('διὰ', 'ADP', 'ADP'), ('πέτρας', 'NOUN', 'NOUN'), ('Κυανέας', 'NOUN', 'NOUN'), ('βασιλῆος', 'NOUN', 'NOUN'), ('ἐφημοσύνῃ', 'ADV', 'ADV'), ('Πελίαο', 'NOUN', 'NOUN'), ('χρύσειον', 'ADJ', 'ADJ'), ('μετὰ', 'ADP', 'ADP'), ('κῶας', 'NOUN', 'NOUN'), ('ἐύζυγον', 'VERB', 'VERB'), ('ἤλασαν', 'VERB', 'VERB'), ('Ἀργώ', 'NOUN', 'NOUN'), ('.', 'PUNCT', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "test_text = \"ἀρχόμενος σέο, Φοῖβε, παλαιγενέων κλέα φωτῶν μνήσομαι, οἳ Πόντοιο κατὰ στόμα καὶ διὰ πέτρας Κυανέας βασιλῆος ἐφημοσύνῃ Πελίαο χρύσειον μετὰ κῶας ἐύζυγον ἤλασαν Ἀργώ.\"\n",
    "doc = nlp(test_text)\n",
    "print(\"Tags\", [(t.text, t.tag_, t.pos_) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(\"nlp_model_angr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it back\n",
    "nlp = spacy.load(\"nlp_model_angr\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('spacy_env')",
   "metadata": {
    "interpreter": {
     "hash": "6948227344bfff8576cc12a6053ffa9e7fa41c58a8be2e4a10b7909366ed531b"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
