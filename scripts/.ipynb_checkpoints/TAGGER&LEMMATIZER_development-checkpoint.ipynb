{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pyconll\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "import sddk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sciencedata.dk username (format '123456@au.dk'): 648597@au.dk\n",
      "sciencedata.dk password: ········\n",
      "connection with shared folder established with you as its owner\n",
      "endpoint variable has been configured to: https://sciencedata.dk/files/SDAM_root/\n"
     ]
    }
   ],
   "source": [
    "conf = sddk.configure(\"SDAM_root\", \"648597@au.dk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[38;5;2m✔ Successfully created model\u001b[0m\n",
      "132846it [00:06, 20486.41it/s]/word2vec_win2.txt\n",
      "\u001b[2K\u001b[38;5;2m✔ Loaded vectors from ../data/word2vec_win2.txt\u001b[0m\n",
      "\u001b[38;5;2m✔ Sucessfully compiled vocab\u001b[0m\n",
      "133518 entries, 132846 vectors\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy init-model el ../models/spacy_grc_model_7_vectors --vectors-loc ../data/word2vec_win2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it into our envoronment\n",
    "nlp = spacy.load(\"../models/spacy_grc_model_7_vectors\")\n",
    "tagger = nlp.create_pipe(\"tagger\")\n",
    "nlp.add_pipe(tagger)\n",
    "optimizer = nlp.begin_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add lookup tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_lemma_lookup = json.load(open(\"../data/ag_lemma_lookup.json\"))\n",
    "ag_lemma_lookup_merged = json.load(open(\"../data/ag_lemma_lookup_merged.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADJ', 'PRON', 'ADP', 'PUNCT', 'DET', 'CCONJ', 'INTJ', 'PART', 'PROPN', 'AUX', 'SCONJ', 'ADV', 'NOUN', 'X', 'NUM', 'VERB']\n"
     ]
    }
   ],
   "source": [
    "print(list(ag_lemma_lookup.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ἐνέργεια'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag_lemma_lookup[\"NOUN\"][\"ἐνεργειῶν\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, this works but there is a substantial number of cases in which there is a wrong tag, e.g. \"VERB\" - in this case the lemmatization is unsuccessful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ἐνέργεια'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag_lemma_lookup_merged[\"ἐνεργειῶν\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nlp.vocab.lookups.remove_table(\"lemma_lookup\")\n",
    "    nlp.vocab.lookups.remove_table(\"lemma_lookup_merged\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let add our table to our model\n",
    "table = nlp.vocab.lookups.add_table(\"lemma_lookup\", ag_lemma_lookup)\n",
    "table = nlp.vocab.lookups.add_table(\"lemma_lookup_merged\", ag_lemma_lookup_merged)\n",
    "# nlp.vocab.lookups.remove_table(\"lemma_exc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'μιμνήσκω'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load it back from the model (to check functionality for future usage)\n",
    "ag_lemma_lookup = nlp.vocab.lookups.get_table(\"lemma_lookup\")\n",
    "ag_lemma_lookup_merged = nlp.vocab.lookups.get_table(\"lemma_lookup_merged\")\n",
    "\n",
    "\n",
    "ag_lemma_lookup[\"VERB\"][\"μνήσομαι\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install greek-accentuation\n",
    "from greek_accentuation.characters import strip_accents\n",
    "from greek_accentuation.syllabify import *\n",
    "from greek_accentuation.accentuation import *\n",
    "\n",
    "\n",
    "\n",
    "def grave_to_acute(string):\n",
    "    GRAVE = \"\\u0300\"\n",
    "    ACUTE = \"\\u0301\"\n",
    "    return unicodedata.normalize(\"NFC\", \"\".join(unicodedata.normalize(\"NFD\", string).replace(GRAVE, ACUTE)))\n",
    "\n",
    "\n",
    "def list_of_possible_accentuations(morph):\n",
    "    try:\n",
    "        if isinstance(morph, str):\n",
    "            morph = strip_accents(morph)\n",
    "            morph = rebreath(morph.lower())\n",
    "            s = syllabify(morph)\n",
    "            morph_vars = []\n",
    "            for accentuation in possible_accentuations(s, default_short=True):\n",
    "                pos, accent = accentuation #add_accentuation(s, accentuation))\n",
    "                final = s[1 - pos:] if pos > 1 else [\"\"]\n",
    "                morph_acc_var = \"\".join(s[:-pos] + [syllable_add_accent(s[-pos], accent)] + final)\n",
    "                morph_vars.append(morph_acc_var)\n",
    "                morph_vars.append(morph_acc_var.capitalize())\n",
    "            return morph_vars\n",
    "        else:\n",
    "            return []\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "def grc_doc_lemmatizer(doc):\n",
    "    for token in doc:\n",
    "        token.lemma_, token.pos_ = lemmatizer(token.text, token.pos_, token.lemma_)\n",
    "    return doc   \n",
    "\n",
    "def apply_nlp(sentences_list):\n",
    "    spacy_docs = []\n",
    "    for sentence in sentences_list:\n",
    "        doc = nlp(sentence)\n",
    "        spacy_docs.append(doc)\n",
    "    return spacy_docs\n",
    "\n",
    "def check_char_validity(token_text, tag):\n",
    "    if re.search(\"\\W\", token_text):\n",
    "        if re.match(\"\\W+$\", token_text):\n",
    "            tag = \"PUNCT\"\n",
    "        else:\n",
    "            if \"’\" not in token_text:\n",
    "                token_text = re.sub(\"\\W\", \"\", token_text)\n",
    "    return token_text, tag\n",
    "\n",
    "# for new tags in specific order...\n",
    "\n",
    "keys = ['PRON', 'ADP', 'PUNCT', 'DET', 'CCONJ', 'INTJ', 'PART', 'ADJ', 'AUX', 'SCONJ', 'ADV', 'NOUN','PROPN', 'VERB','X', 'NUM']\n",
    " \n",
    "def check_other_tags(token_text, tag):\n",
    "    lemma = token_text\n",
    "    match = False\n",
    "    # for new tags in specific order...\n",
    "    for new_tag in keys:\n",
    "        try:\n",
    "            lemma = ag_lemma_lookup[new_tag][token_text]\n",
    "            tag = new_tag\n",
    "            match = True\n",
    "            break\n",
    "        except:\n",
    "            pass\n",
    "    return lemma, tag, match\n",
    "\n",
    "\n",
    "def lemmatizer_v1(token_text, tag, old_lemma=None):\n",
    "    if (old_lemma==None) or (token_text == old_lemma):\n",
    "        lemma = token_text.lower() # start with assigning the word as it stands\n",
    "        try:\n",
    "            lemma = ag_lemma_lookup[tag][token_text]\n",
    "        except:\n",
    "            try:\n",
    "                lemma = ag_lemma_lookup_merged[token_text]\n",
    "            except:\n",
    "                try:\n",
    "                    lemma = ag_lemma_lookup[tag][grave_to_acute(token_text)]\n",
    "                except:\n",
    "                    try:\n",
    "                        lemma = ag_lemma_lookup_merged[grave_to_acute(token_text)]\n",
    "                    except:\n",
    "                        morph_vars = list_of_possible_accentuations(token_text)\n",
    "                        for var in morph_vars:\n",
    "                            try:\n",
    "                                lemma = ag_lemma_lookup[tag][var] \n",
    "                                break\n",
    "                            except:\n",
    "                                try:\n",
    "                                    lemma = ag_lemma_lookup_merged[var]\n",
    "                                    break\n",
    "                                except:\n",
    "                                    pass\n",
    "        return lemma, tag\n",
    "    else:\n",
    "        return old_lemma, tag\n",
    "    \n",
    "def lemmatizer(token_text, tag, old_lemma=None):\n",
    "    if (old_lemma==None) or (token_text == old_lemma):\n",
    "        token_text, tag = check_char_validity(token_text, tag)\n",
    "        lemma = token_text.lower() # start with assigning the word as it stands\n",
    "        try:\n",
    "            lemma = ag_lemma_lookup[tag][token_text]\n",
    "        except:\n",
    "            try:\n",
    "                lemma = ag_lemma_lookup[tag][grave_to_acute(token_text)]\n",
    "            except:\n",
    "                if check_other_tags(token_text, tag)[2] == True:\n",
    "                    lemma, tag, match = check_other_tags(token_text, tag)\n",
    "                else:\n",
    "                        morph_vars = list_of_possible_accentuations(token_text)\n",
    "                        for var in morph_vars:\n",
    "                            try:\n",
    "                                try:\n",
    "                                    lemma = ag_lemma_lookup[tag][var] \n",
    "                                    break\n",
    "                                except:\n",
    "                                    a, b, match = check_other_tags(var, tag)\n",
    "                                    if match ==True:\n",
    "                                        lemma, tag, match = check_other_tags(var, tag)\n",
    "                                        break\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "        return lemma, tag\n",
    "    else:\n",
    "        return old_lemma, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ἀπό', 'ADP')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer('ἀπʼ', 'PUNCT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('δι’', 'ADP')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer(\"δι’\", \"ADP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Παῦλος', 'NOUN')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer(\"ΠΑΥΛΟΣ\", \"VERB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('βίβλος', 'NOUN')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer(\"ΒΙΒΛΟΣ\", \"ADJ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Ἰησοῦς', 'NOUN')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer(\"Ἰησοῦ\", \"NOUN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('βιββιβλος', 'NOUN')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nonexistent word\n",
    "lemmatizer(\"ΒΙΒΒΙΒΛΟΣ\", \"NOUN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ἐνέργεια', 'NOUN')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it works even with wrong POS-tag\n",
    "lemmatizer(\"ἐνεργειῶν\", \"VERB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('θεός', 'NOUN')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer(\"θεοῦ.\", \"PUNCT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training our own model with gold-parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Ancient_Greek-Perseus/master/grc_perseus-ud-train.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Ancient_Greek-Perseus/master/grc_perseus-ud-dev.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Ancient_Greek-PROIEL/master/grc_proiel-ud-train.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Ancient_Greek-PROIEL/master/grc_proiel-ud-dev.conllu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_encoding(string):\n",
    "    return unicodedata.normalize(\"NFC\", string)\n",
    "v_ud = \"’\"\n",
    "v_agt = \"ʼ\"\n",
    "\n",
    "v_ud == v_agt # because of that, we will everywhere use the agt version, since it does not confuse the word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagged_data_from_url(url):\n",
    "    corpus = pyconll.load.iter_from_url(url)\n",
    "    tagged_data = []\n",
    "    for sentence in corpus:\n",
    "        words, tags, lemmata = [], [], []\n",
    "        for token in sentence:\n",
    "            words.append(normalize_encoding(re.sub(v_ud, v_agt, token.form)))\n",
    "            tags.append(token.upos)\n",
    "            lemmata.append(normalize_encoding(token.lemma))\n",
    "        if \"-dev.\" in url:\n",
    "            tagged_data.append((normalize_encoding(re.sub(v_ud, v_agt, sentence.text)), {\"words\" : words, \"tags\" : tags, \"lemmata\" : lemmata}))\n",
    "        else:\n",
    "            tagged_data.append((normalize_encoding(re.sub(v_ud, v_agt, sentence.text)), {\"words\" : words, \"tags\" : tags}))\n",
    "    print(\"File name: {0}; number of sentences: {1}\".format(url.rpartition(\"/\")[2], len(tagged_data)))\n",
    "    return tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name: grc_perseus-ud-train.conllu; number of sentences: 11476\n",
      "File name: grc_perseus-ud-dev.conllu; number of sentences: 1137\n",
      "File name: grc_proiel-ud-train.conllu; number of sentences: 15014\n",
      "File name: grc_proiel-ud-dev.conllu; number of sentences: 1019\n"
     ]
    }
   ],
   "source": [
    "perseus_train = tagged_data_from_url(urls[0])\n",
    "perseus_dev = tagged_data_from_url(urls[1])\n",
    "proiel_train = tagged_data_from_url(urls[2])\n",
    "proiel_dev = tagged_data_from_url(urls[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ἐρᾷ μὲν ἁγνὸς οὐρανὸς τρῶσαι χθόνα, ἔρως δὲ γαῖαν λαμβάνει γάμου τυχεῖν·', {'words': ['ἐρᾷ', 'μὲν', 'ἁγνὸς', 'οὐρανὸς', 'τρῶσαι', 'χθόνα', ',', 'ἔρως', 'δὲ', 'γαῖαν', 'λαμβάνει', 'γάμου', 'τυχεῖν', '·'], 'tags': ['VERB', 'ADV', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'PUNCT', 'NOUN', 'CCONJ', 'NOUN', 'VERB', 'NOUN', 'VERB', 'PUNCT']})\n"
     ]
    }
   ],
   "source": [
    "print(perseus_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = perseus_train + proiel_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26490"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens in proiel_dev: 13652\n",
      "tokens in perseus_dev: 22135\n"
     ]
    }
   ],
   "source": [
    "proiel_dev_len = sum([len(sent[1][\"tags\"]) for sent in proiel_dev])\n",
    "print(\"tokens in proiel_dev: \" + str(proiel_dev_len))\n",
    "\n",
    "perseus_dev_len = sum([len(sent[1][\"tags\"]) for sent in perseus_dev])\n",
    "print(\"tokens in perseus_dev: \" + str(perseus_dev_len))\n",
    "total_len = proiel_dev_len + perseus_dev_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tests = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add lemmatizer to the pipeline\n",
    "#nlp.remove_pipe(\"grc_doc_lemmatizer\")\n",
    "nlp.add_pipe(grc_doc_lemmatizer, \"grc_doc_lemmatizer\", after=\"tagger\") # grc_doc_lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Losses': {'tagger': 56364.95995467901}, 'proiel_tags_correct%': 95.01171989452095, 'proiel_lemmata_correct%': 95.29739232346908, 'perseus_tags_correct%': 87.53557714027558, 'perseus_lemmata_correct%': 87.06573300203297}\n",
      "{'Losses': {'tagger': 46929.60791128874}, 'proiel_tags_correct%': 95.89071198359214, 'proiel_lemmata_correct%': 95.35599179607384, 'perseus_tags_correct%': 87.88344251185904, 'perseus_lemmata_correct%': 87.04766207363903}\n",
      "{'Losses': {'tagger': 41977.65289503336}, 'proiel_tags_correct%': 95.407266334603, 'proiel_lemmata_correct%': 95.45121593905654, 'perseus_tags_correct%': 88.44364129207139, 'perseus_lemmata_correct%': 87.21933589338153}\n",
      "{'Losses': {'tagger': 38295.55491784215}, 'proiel_tags_correct%': 96.08848520363317, 'proiel_lemmata_correct%': 95.43656607090536, 'perseus_tags_correct%': 88.00090354641969, 'perseus_lemmata_correct%': 87.03410887734357}\n",
      "{'Losses': {'tagger': 35758.74683788419}, 'proiel_tags_correct%': 95.64166422502197, 'proiel_lemmata_correct%': 95.49516554351011, 'perseus_tags_correct%': 88.48430088095776, 'perseus_lemmata_correct%': 87.22385362548}\n",
      "{'Losses': {'tagger': 33840.40595907532}, 'proiel_tags_correct%': 96.22033401699385, 'proiel_lemmata_correct%': 95.49516554351011, 'perseus_tags_correct%': 88.28552066862436, 'perseus_lemmata_correct%': 87.1606053761012}\n",
      "{'Losses': {'tagger': 32095.809285717492}, 'proiel_tags_correct%': 96.31555815997656, 'proiel_lemmata_correct%': 95.44389100498096, 'perseus_tags_correct%': 88.35328665010165, 'perseus_lemmata_correct%': 87.1922295007906}\n",
      "{'Losses': {'tagger': 30854.25888666138}, 'proiel_tags_correct%': 95.9419865221213, 'proiel_lemmata_correct%': 95.48784060943451, 'perseus_tags_correct%': 88.96318048339734, 'perseus_lemmata_correct%': 87.25547775016942}\n",
      "{'Losses': {'tagger': 29489.01731290482}, 'proiel_tags_correct%': 96.42543217111046, 'proiel_lemmata_correct%': 95.51714034573689, 'perseus_tags_correct%': 88.42557036367744, 'perseus_lemmata_correct%': 87.1606053761012}\n",
      "{'Losses': {'tagger': 28476.38776820898}, 'proiel_tags_correct%': 96.15440961031351, 'proiel_lemmata_correct%': 95.49516554351011, 'perseus_tags_correct%': 88.59724418341992, 'perseus_lemmata_correct%': 87.1244635193133}\n",
      "{'Losses': {'tagger': 27686.765684477054}, 'proiel_tags_correct%': 96.3741576325813, 'proiel_lemmata_correct%': 95.49516554351011, 'perseus_tags_correct%': 88.99480460808674, 'perseus_lemmata_correct%': 87.21481816128303}\n",
      "{'Losses': {'tagger': 26702.884508642368}, 'proiel_tags_correct%': 96.23498388514503, 'proiel_lemmata_correct%': 95.53179021388807, 'perseus_tags_correct%': 88.44364129207139, 'perseus_lemmata_correct%': 87.11091032301785}\n",
      "{'Losses': {'tagger': 26114.314855716657}, 'proiel_tags_correct%': 96.35950776443012, 'proiel_lemmata_correct%': 95.49516554351011, 'perseus_tags_correct%': 88.95866275129885, 'perseus_lemmata_correct%': 87.23740682177547}\n",
      "{'Losses': {'tagger': 25574.5498828087}, 'proiel_tags_correct%': 96.13975974216233, 'proiel_lemmata_correct%': 95.51714034573689, 'perseus_tags_correct%': 88.74632934266998, 'perseus_lemmata_correct%': 87.1922295007906}\n",
      "{'Losses': {'tagger': 24738.726235747337}, 'proiel_tags_correct%': 96.47670670963961, 'proiel_lemmata_correct%': 95.52446527981249, 'perseus_tags_correct%': 88.99480460808674, 'perseus_lemmata_correct%': 87.18771176869211}\n",
      "{'Losses': {'tagger': 24307.362325702794}, 'proiel_tags_correct%': 96.43275710518606, 'proiel_lemmata_correct%': 95.52446527981249, 'perseus_tags_correct%': 88.48881861305624, 'perseus_lemmata_correct%': 87.14705217980574}\n",
      "{'Losses': {'tagger': 23920.308644185774}, 'proiel_tags_correct%': 96.17638441254029, 'proiel_lemmata_correct%': 95.5098154116613, 'perseus_tags_correct%': 89.29297492658685, 'perseus_lemmata_correct%': 87.17867630449514}\n",
      "{'Losses': {'tagger': 23543.56598253781}, 'proiel_tags_correct%': 96.57193085262233, 'proiel_lemmata_correct%': 95.48784060943451, 'perseus_tags_correct%': 88.73729387847301, 'perseus_lemmata_correct%': 87.15608764400271}\n",
      "{'Losses': {'tagger': 23017.382916590548}, 'proiel_tags_correct%': 96.40345736888368, 'proiel_lemmata_correct%': 95.55376501611485, 'perseus_tags_correct%': 89.25231533770047, 'perseus_lemmata_correct%': 87.1922295007906}\n",
      "{'Losses': {'tagger': 22533.66998050129}, 'proiel_tags_correct%': 96.42543217111046, 'proiel_lemmata_correct%': 95.53179021388807, 'perseus_tags_correct%': 89.39688276485205, 'perseus_lemmata_correct%': 87.16964084029817}\n",
      "{'Losses': {'tagger': 22403.837807178497}, 'proiel_tags_correct%': 96.43275710518606, 'proiel_lemmata_correct%': 95.49516554351011, 'perseus_tags_correct%': 89.01287553648068, 'perseus_lemmata_correct%': 87.10187485882088}\n",
      "{'Losses': {'tagger': 22105.197869144846}, 'proiel_tags_correct%': 96.44740697333724, 'proiel_lemmata_correct%': 95.48051567535893, 'perseus_tags_correct%': 88.94962728710188, 'perseus_lemmata_correct%': 87.1606053761012}\n",
      "{'Losses': {'tagger': 21756.431068399645}, 'proiel_tags_correct%': 96.34485789627894, 'proiel_lemmata_correct%': 95.56841488426603, 'perseus_tags_correct%': 89.36074090806414, 'perseus_lemmata_correct%': 87.1967472328891}\n",
      "{'Losses': {'tagger': 21313.53678219393}, 'proiel_tags_correct%': 96.3888075007325, 'proiel_lemmata_correct%': 95.54644008203927, 'perseus_tags_correct%': 89.7131240117461, 'perseus_lemmata_correct%': 87.17867630449514}\n",
      "{'Losses': {'tagger': 20961.601952746452}, 'proiel_tags_correct%': 96.60123058892471, 'proiel_lemmata_correct%': 95.51714034573689, 'perseus_tags_correct%': 88.9405918229049, 'perseus_lemmata_correct%': 87.15608764400271}\n"
     ]
    }
   ],
   "source": [
    "n_iter = 25\n",
    "\n",
    "model_tests = []\n",
    "for i in range(n_iter):\n",
    "    \n",
    "    ### TRAIN THE MODEL\n",
    "    random.shuffle(train_data)\n",
    "    losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(texts, annotations, drop=0.3, sgd=optimizer, losses=losses)\n",
    "    \n",
    "    ### TEST THE MODEL\n",
    "    \n",
    "    model_test = {}\n",
    "    model_test[\"Losses\"] = losses\n",
    "    proiel_tags_correct = 0\n",
    "    proiel_lemmata_correct = 0\n",
    "    for sent in proiel_dev:\n",
    "        doc = nlp(sent[0])\n",
    "        predicted_tags = [token.pos_ for token in doc]\n",
    "        predicted_lemmata = [token.lemma_ for token in doc]\n",
    "        #predicted_lemmata = [lemmatizer(token.text, token.pos_) for token in doc]\n",
    "        given_tags = sent[1][\"tags\"]\n",
    "        given_lemmata = sent[1][\"lemmata\"]\n",
    "        proiel_tags_correct += len([p for p, g in zip(predicted_tags, given_tags) if p ==g])\n",
    "        proiel_lemmata_correct += len([p for p, g in zip(predicted_lemmata, given_lemmata) if p ==g])\n",
    "    model_test[\"proiel_tags_correct%\"] = (proiel_tags_correct / proiel_dev_len) * 100\n",
    "    model_test[\"proiel_lemmata_correct%\"] = (proiel_lemmata_correct / proiel_dev_len) * 100\n",
    "\n",
    "    perseus_tags_correct = 0\n",
    "    perseus_lemmata_correct = 0\n",
    "    for sent in perseus_dev:\n",
    "        doc = nlp(sent[0])\n",
    "        predicted_tags = [token.pos_ for token in doc]\n",
    "        predicted_lemmata = [token.lemma_ for token in doc]\n",
    "        #predicted_lemmata = [lemmatizer(token.text, token.pos_) for token in doc]\n",
    "        given_tags = sent[1][\"tags\"]\n",
    "        given_lemmata = sent[1][\"lemmata\"]\n",
    "        perseus_tags_correct += len([p for p, g in zip(predicted_tags, given_tags) if p ==g])\n",
    "        perseus_lemmata_correct += len([p for p, g in zip(predicted_lemmata, given_lemmata) if p ==g])\n",
    "    model_test[\"perseus_tags_correct%\"] = (perseus_tags_correct / perseus_dev_len) * 100\n",
    "    model_test[\"perseus_lemmata_correct%\"] = (perseus_lemmata_correct / perseus_dev_len) * 100\n",
    "    print(model_test)\n",
    "    model_tests.append(model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['πολλῷ', 'γὰρ', 'ὕστερον', 'ἔτι', 'καὶ', 'τῶν', 'Τρωικῶν', 'γενόμενος', 'οὐδαμοῦ', 'τοὺς', 'ξύμπαντας', 'ὠνόμασεν', ',', 'οὐδ̓', 'ἄλλους', 'ἢ', 'τοὺς', 'μετ̓', 'Ἀχιλλέως', 'ἐκ', 'τῆς', 'Φθιώτιδος', ',', 'οἵπερ', 'καὶ', 'πρῶτοι', 'Ἕλληνες', 'ἦσαν', ',', 'Δαναοὺς', 'δὲ', 'ἐν', 'τοῖς', 'ἔπεσι', 'καὶ', 'Ἀργείους', 'καὶ', 'Ἀχαιοὺς', 'ἀνακαλεῖ', '.'], [('πολλῷ', 'πολύς'), ('γὰρ', 'γάρ'), ('ὕστερον', 'ὕστερος'), ('ἔτι', 'ἔτι'), ('καὶ', 'καί'), ('τῶν', 'ὁ'), ('Τρωικῶν', 'Τρωικός'), ('γενόμενος', 'γίγνομαι'), ('οὐδαμοῦ', 'οὐδαμοῦ'), ('τοὺς', 'ὁ'), ('ξύμπαντας', 'σύμπας'), ('ὠνόμασεν', 'ὀνομάζω'), (',', ','), ('οὐδ̓', 'οὐδ'), ('ἄλλους', 'ἄλλος'), ('ἢ', 'ἤ'), ('τοὺς', 'ὁ'), ('μετ̓', 'μετ'), ('Ἀχιλλέως', 'Ἀχιλλεύς'), ('ἐκ', 'ἐκ'), ('τῆς', 'ὁ'), ('Φθιώτιδος', 'Φθιῶτις'), (',', ','), ('οἵπερ', 'ὅσπερ'), ('καὶ', 'καί'), ('πρῶτοι', 'πρῶτος'), ('Ἕλληνες', 'Ἕλλην'), ('ἦσαν', 'εἰμί'), (',', ','), ('Δαναοὺς', 'Δαναοί'), ('δὲ', 'δέ'), ('ἐν', 'ἐν'), ('τοῖς', 'ὁ'), ('ἔπεσι', 'ἔπος'), ('καὶ', 'καί'), ('Ἀργείους', 'Ἀργεῖος'), ('καὶ', 'καί'), ('Ἀχαιοὺς', 'Ἀχαιός'), ('ἀνακαλεῖ.', 'ἀνακαλέω')]), (['καὶ', 'Πολυκράτης', 'Σάμου', 'τυραννῶν', 'ἐπὶ', 'Καμβύσου', 'ναυτικῷ', 'ἰσχύων', 'ἄλλας', 'τε', 'τῶν', 'νήσων', 'ὑπηκόους', 'ἐποιήσατο', 'καὶ', 'Ῥήνειαν', 'ἑλὼν', 'ἀνέθηκε', 'τῷ', 'Ἀπόλλωνι', 'τῷ', 'Δηλίῳ', '.'], [('καὶ', 'καί'), ('Πολυκράτης', 'Πολυκράτης'), ('Σάμου', 'Σάμος'), ('τυραννῶν', 'τύραννος'), ('ἐπὶ', 'ἐπί'), ('Καμβύσου', 'καμβύσου'), ('ναυτικῷ', 'ναυτικός'), ('ἰσχύων', 'ἰσχύω'), ('ἄλλας', 'ἄλλος'), ('τε', 'τε'), ('τῶν', 'ὁ'), ('νήσων', 'νῆσος'), ('ὑπηκόους', 'ὑπήκοος'), ('ἐποιήσατο', 'ποιέω'), ('καὶ', 'καί'), ('Ῥήνειαν', 'ῥαίνω'), ('ἑλὼν', 'αἱρέω'), ('ἀνέθηκε', 'ἀνατίθημι'), ('τῷ', 'ὁ'), ('Ἀπόλλωνι', 'Ἀπόλλων'), ('τῷ', 'ὁ'), ('Δηλίῳ.', 'Δήλιος')]), (['τῇ', 'δὲ', 'αὐτῇ', 'ἡμέρᾳ', 'αὐτοῖς', 'ξυνέβη', 'καὶ', 'τοὺς', 'τὴν', 'Ἐπίδαμνον', 'πολιορκοῦντας', 'παραστήσασθαι', 'ὁμολογίᾳ', 'ὥστε', 'τοὺς', 'μὲν', 'ἐπήλυδας', 'ἀποδόσθαι', ',', 'Κορινθίους', 'δὲ', 'δήσαντας', 'ἔχειν', 'ἕως', 'ἂν', 'ἄλλο', 'τι', 'δόξῃ', '.'], [('τῇ', 'ὁ'), ('δὲ', 'δέ'), ('αὐτῇ', 'αὐτός'), ('ἡμέρᾳ', 'ἡμέρα'), ('αὐτοῖς', 'αὐτός'), ('ξυνέβη', 'συμβαίνω'), ('καὶ', 'καί'), ('τοὺς', 'ὁ'), ('τὴν', 'ὁ'), ('Ἐπίδαμνον', 'ἐπίδαμνον'), ('πολιορκοῦντας', 'πολιορκοῦντας'), ('παραστήσασθαι', 'παρίστημι'), ('ὁμολογίᾳ', 'ὁμολογία'), ('ὥστε', 'ὥστε'), ('τοὺς', 'ὁ'), ('μὲν', 'μέν'), ('ἐπήλυδας', 'ἔπηλυς'), ('ἀποδόσθαι', 'ἀποδίδωμι'), (',', ','), ('Κορινθίους', 'Κορίνθιος'), ('δὲ', 'δέ'), ('δήσαντας', 'δήσαντας'), ('ἔχειν', 'ἔχω'), ('ἕως', 'ἕως'), ('ἂν', 'ἄν'), ('ἄλλο', 'ἄλλος'), ('τι', 'τίς'), ('δόξῃ.', 'δοκέω')])]\n"
     ]
    }
   ],
   "source": [
    "# The reason why the performance on perseus is not so good is actually caused by inperfect tokenization \n",
    "# -> implying that our sentences are of different length than the sentences in the test data\n",
    "\n",
    "all_tags_len = 0\n",
    "inconsistencies = []\n",
    "perseus_tags_correct = 0\n",
    "perseus_lemmata_correct = 0\n",
    "for sent in perseus_dev:\n",
    "        doc = nlp(sent[0])\n",
    "        predicted_tags = [token.pos_ for token in doc]\n",
    "        all_tags_len = all_tags_len + len(predicted_tags)\n",
    "        predicted_lemmata = [token.lemma_ for token in doc]\n",
    "        #predicted_lemmata = [lemmatizer(token.text, token.pos_) for token in doc]\n",
    "        given_tags = sent[1][\"tags\"]\n",
    "        given_lemmata = sent[1][\"lemmata\"]\n",
    "        if len(predicted_lemmata) != len(given_lemmata):\n",
    "            inconsistencies.append((sent[1][\"words\"], [(token.text, token.lemma_) for token in doc]))\n",
    "        perseus_tags_correct += len([p for p, g in zip(predicted_tags, given_tags) if p ==g])\n",
    "        perseus_lemmata_correct += len([p for p, g in zip(predicted_lemmata, given_lemmata) if p ==g])\n",
    "model_test[\"perseus_tags_correct%\"] = (perseus_tags_correct / perseus_dev_len) * 100\n",
    "model_test[\"perseus_lemmata_correct%\"] = (perseus_lemmata_correct / perseus_dev_len) * 100\n",
    "print(inconsistencies[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22107"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tags_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'proiel_tags_correct%': 96.60123058892471,\n",
       " 'proiel_lemmata_correct%': 95.51714034573689,\n",
       " 'perseus_tags_correct%': 88.9405918229049,\n",
       " 'perseus_lemmata_correct%': 87.15608764400271}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMPARE WITH OUR FIRST MODEL\n",
    "nlp_1 = spacy.load(\"../models/spacy_grc_model_1\")\n",
    "\n",
    "model_test = {}\n",
    "proiel_tags_correct = 0\n",
    "proiel_lemmata_correct = 0\n",
    "for sent in proiel_dev:\n",
    "    doc = nlp(sent[0])\n",
    "    predicted_tags = [token.pos_ for token in doc]\n",
    "    all_tags_len = all_tags_len + len(predicted_tags)\n",
    "    predicted_lemmata = [token.lemma_ for token in doc]\n",
    "    given_tags = sent[1][\"tags\"]\n",
    "    given_lemmata = sent[1][\"lemmata\"]\n",
    "    proiel_tags_correct += len([p for p, g in zip(predicted_tags, given_tags) if p ==g])\n",
    "    proiel_lemmata_correct += len([p for p, g in zip(predicted_lemmata, given_lemmata) if p ==g])\n",
    "model_test[\"proiel_tags_correct%\"] = (proiel_tags_correct / proiel_dev_len) * 100\n",
    "model_test[\"proiel_lemmata_correct%\"] = (proiel_lemmata_correct / proiel_dev_len) * 100\n",
    "\n",
    "perseus_tags_correct = 0\n",
    "perseus_lemmata_correct = 0\n",
    "for sent in perseus_dev:\n",
    "    doc = nlp(sent[0])\n",
    "    predicted_tags = [token.pos_ for token in doc]\n",
    "    all_tags_len = all_tags_len + len(predicted_tags)\n",
    "    predicted_lemmata = [token.lemma_ for token in doc]\n",
    "    given_tags = sent[1][\"tags\"]\n",
    "    given_lemmata = sent[1][\"lemmata\"]\n",
    "    perseus_tags_correct += len([p for p, g in zip(predicted_tags, given_tags) if p ==g])\n",
    "    perseus_lemmata_correct += len([p for p, g in zip(predicted_lemmata, given_lemmata) if p ==g])\n",
    "model_test[\"perseus_tags_correct%\"] = (perseus_tags_correct / perseus_dev_len) * 100\n",
    "model_test[\"perseus_lemmata_correct%\"] = (perseus_lemmata_correct / perseus_dev_len) * 100\n",
    "\n",
    "model_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our results with first model:\n",
    "```\n",
    "{'proiel_tags_correct%': 87.90653384119544,\n",
    " 'proiel_lemmata_correct%': 85.62115440961031,\n",
    " 'perseus_tags_correct%': 85.05534221820646,\n",
    " 'perseus_lemmata_correct%': 82.66997967020556}\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.remove_pipe('grc_doc_lemmatizer')\n",
    "nlp.to_disk(\"../models/spacy_grc_model_7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGT = sddk.read_file(\"SDAM_data/AGT/AGT_preprocessed_20201209.json\", \"df\", conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"../models/spacy_grc_model_7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(grc_doc_lemmatizer, \"grc_doc_lemmatizer\", after=\"tagger\") # grc_doc_lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ΠΑΥΛΟΣ', 'NOUN', 'Παῦλος'), ('ἀπόστολος', 'NOUN', 'ἀπόστολος'), (',', 'PUNCT', ','), ('οὐκ', 'ADV', 'οὐ'), ('ἀπʼ', 'ADP', 'ἀπό'), ('ἀνθρώπων', 'NOUN', 'ἄνθρωπος'), ('οὐδὲ', 'CCONJ', 'οὐδέ'), ('διʼ', 'ADP', 'διά'), ('ἀνθρώπου', 'NOUN', 'ἄνθρωπος'), ('ἀλλὰ', 'CCONJ', 'ἀλλά'), ('διὰ', 'ADP', 'διά'), ('Ἰησοῦ', 'PROPN', 'Ἰησοῦς'), ('Χριστοῦ', 'PROPN', 'Χριστός'), ('καὶ', 'CCONJ', 'καί'), ('θεοῦ', 'NOUN', 'θεός'), ('πατρὸς', 'NOUN', 'πατήρ'), ('τοῦ', 'DET', 'ὁ'), ('ἐγείραντος', 'VERB', 'ἐγείρω'), ('αὐτὸν', 'PRON', 'αὐτός'), ('ἐκ', 'ADP', 'ἐκ'), ('νεκρῶν', 'NOUN', 'νεκρός'), (',', 'PUNCT', ','), ('καὶ', 'CCONJ', 'καί'), ('οἱ', 'DET', 'ὁ'), ('σὺν', 'ADP', 'σύν'), ('ἐμοὶ', 'PRON', 'ἐγώ'), ('πάντες', 'ADJ', 'πᾶς'), ('ἀδελφοί', 'NOUN', 'ἀδελφός'), (',', 'PUNCT', ','), ('ταῖς', 'DET', 'ὁ'), ('ἐκκλησίαις', 'NOUN', 'ἐκκλησία'), ('τῆς', 'DET', 'ὁ'), ('Γαλατίας', 'NOUN', 'Γαλατία'), ('·', 'PUNCT', '·')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(AGT[AGT[\"author_id\"]==\"tlg0031paul\"].iloc[3][\"sentences\"][0])\n",
    "print([(token.text, token.pos_, token.lemma_) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ΠΑΥΛΟΣ', 'NOUN', 'Παῦλος'), ('ἀπόστολος', 'NOUN', 'ἀπόστολος'), (',', 'PUNCT', ','), ('οὐκ', 'ADV', 'οὐ'), ('ἀπʼ', 'ADP', 'ἀπό'), ('ἀνθρώπων', 'NOUN', 'ἄνθρωπος'), ('οὐδὲ', 'CCONJ', 'οὐδέ'), ('διʼ', 'ADP', 'διά'), ('ἀνθρώπου', 'NOUN', 'ἄνθρωπος'), ('ἀλλὰ', 'CCONJ', 'ἀλλά'), ('διὰ', 'ADP', 'διά'), ('Ἰησοῦ', 'PROPN', 'Ἰησοῦς'), ('Χριστοῦ', 'PROPN', 'Χριστός'), ('καὶ', 'CCONJ', 'καί'), ('θεοῦ', 'NOUN', 'θεός'), ('πατρὸς', 'NOUN', 'πατήρ'), ('τοῦ', 'DET', 'ὁ'), ('ἐγείραντος', 'VERB', 'ἐγείρω'), ('αὐτὸν', 'PRON', 'αὐτός'), ('ἐκ', 'ADP', 'ἐκ'), ('νεκρῶν', 'NOUN', 'νεκρός'), (',', 'PUNCT', ','), ('καὶ', 'CCONJ', 'καί'), ('οἱ', 'DET', 'ὁ'), ('σὺν', 'ADP', 'σύν'), ('ἐμοὶ', 'PRON', 'ἐγώ'), ('πάντες', 'ADJ', 'πᾶς'), ('ἀδελφοί', 'NOUN', 'ἀδελφός'), (',', 'PUNCT', ','), ('ταῖς', 'DET', 'ὁ'), ('ἐκκλησίαις', 'NOUN', 'ἐκκλησία'), ('τῆς', 'DET', 'ὁ'), ('Γαλατίας', 'NOUN', 'Γαλατία'), ('·', 'PUNCT', '·')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(AGT[AGT[\"author_id\"]==\"tlg0031paul\"].iloc[3][\"sentences\"][0])\n",
    "print([(token.text, token.pos_, token.lemma_) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('πᾶσα', 'DET', 'πᾶς'), ('τέχνη', 'NOUN', 'τέχνη'), ('καὶ', 'CCONJ', 'καί'), ('πᾶσα', 'ADJ', 'πᾶς'), ('μέθοδος', 'NOUN', 'μέθοδος'), (',', 'PUNCT', ','), ('ὁμοίως', 'ADV', 'ὁμοίως'), ('δὲ', 'CCONJ', 'δέ'), ('πρᾶξίς', 'NOUN', 'πρᾶξις'), ('τε', 'PART', 'τε'), ('καὶ', 'CCONJ', 'καί'), ('προαίρεσις', 'NOUN', 'προαίρεσις'), (',', 'PUNCT', ','), ('ἀγαθοῦ', 'ADJ', 'ἀγαθός'), ('τινὸς', 'PRON', 'τίς'), ('ἐφίεσθαι', 'VERB', 'ἐφίημι'), ('δοκεῖ', 'VERB', 'δοκέω'), ('·', 'PUNCT', '·')]\n",
      "[('διὸ', 'ADV', 'διό'), ('καλῶς', 'ADV', 'καλῶς'), ('ἀπεφήναντο', 'VERB', 'ἀποφαίνω'), ('τἀγαθόν', 'ADJ', 'ἀγαθός'), (',', 'PUNCT', ','), ('οὗ', 'ADV', 'οὗ'), ('πάντʼ', 'ADJ', 'πᾶς'), ('ἐφίεται', 'VERB', 'ἐφίημι'), ('.', 'PUNCT', '.')]\n",
      "[('διαφορὰ', 'NOUN', 'διαφορά'), ('δέ', 'ADV', 'δέ'), ('τις', 'PRON', 'τίς'), ('φαίνεται', 'VERB', 'φαίνω'), ('τῶν', 'DET', 'ὁ'), ('τελῶν', 'NOUN', 'τέλος'), ('·', 'PUNCT', '·')]\n",
      "[('τὰ', 'PRON', 'ὁ'), ('μὲν', 'PART', 'μέν'), ('γάρ', 'PART', 'γάρ'), ('εἰσιν', 'VERB', 'εἰμί'), ('ἐνέργειαι', 'NOUN', 'ἐνέργεια'), (',', 'PUNCT', ','), ('τὰ', 'DET', 'ὁ'), ('δὲ', 'CCONJ', 'δέ'), ('παρʼ', 'ADP', 'παρά'), ('αὐτὰς', 'ADJ', 'αὐτός'), ('ἔργα', 'NOUN', 'ἔργον'), ('τινά', 'PRON', 'τις'), ('.', 'PUNCT', '.')]\n",
      "[('ὧν', 'PRON', 'ὅς'), ('δʼ', 'ADV', 'δέ'), ('εἰσὶ', 'AUX', 'εἰμί'), ('τέλη', 'NOUN', 'τέλος'), ('τινὰ', 'DET', 'τὶς'), ('παρὰ', 'ADP', 'παρά'), ('τὰς', 'DET', 'ὁ'), ('πράξεις', 'NOUN', 'πρᾶξις'), (',', 'PUNCT', ','), ('ἐν', 'ADP', 'ἐν'), ('τούτοις', 'PRON', 'οὗτος'), ('βελτίω', 'ADJ', 'βελτίων'), ('πέφυκε', 'VERB', 'φύω'), ('τῶν', 'DET', 'ὁ'), ('ἐνεργειῶν', 'NOUN', 'ἐνέργεια'), ('τὰ', 'DET', 'ὁ'), ('ἔργα', 'NOUN', 'ἔργον'), ('.', 'PUNCT', '.')]\n",
      "[('πολλῶν', 'ADJ', 'πολύς'), ('δὲ', 'ADV', 'δέ'), ('πράξεων', 'NOUN', 'πρᾶξις'), ('οὐσῶν', 'AUX', 'εἰμί'), ('καὶ', 'CCONJ', 'καί'), ('τεχνῶν', 'NOUN', 'τέχνη'), ('καὶ', 'CCONJ', 'καί'), ('ἐπιστημῶν', 'NOUN', 'ἐπιστήμη'), ('πολλὰ', 'ADJ', 'πολύς'), ('γίνεται', 'VERB', 'γίγνομαι'), ('καὶ', 'ADV', 'καί'), ('τὰ', 'DET', 'ὁ'), ('τέλη', 'NOUN', 'τέλος'), ('·', 'PUNCT', '·')]\n",
      "[('ἰατρικῆς', 'ADJ', 'ἰατρικός'), ('μὲν', 'PART', 'μέν'), ('γὰρ', 'PART', 'γάρ'), ('ὑγίεια', 'NOUN', 'ὑγίεια'), (',', 'PUNCT', ','), ('ναυπηγικῆς', 'ADJ', 'ναυπηγικός'), ('δὲ', 'CCONJ', 'δέ'), ('πλοῖον', 'NOUN', 'πλοῖον'), (',', 'PUNCT', ','), ('στρατηγικῆς', 'ADJ', 'στρατηγικός'), ('δὲ', 'PART', 'δέ'), ('νίκη', 'NOUN', 'νίκη'), (',', 'PUNCT', ','), ('οἰκονομικῆς', 'ADJ', 'οἰκονομικός'), ('δὲ', 'PART', 'δέ'), ('πλοῦτος', 'NOUN', 'πλοῦτος'), ('.', 'PUNCT', '.')]\n",
      "[('ὅσαι', 'PRON', 'ὅσος'), ('δʼ', 'ADV', 'δέ'), ('εἰσὶ', 'AUX', 'εἰμί'), ('τῶν', 'DET', 'ὁ'), ('τοιούτων', 'ADJ', 'τοιοῦτος'), ('ὑπὸ', 'ADP', 'ὑπό'), ('μίαν', 'ADJ', 'εἷς'), ('τινὰ', 'PRON', 'τίς'), ('δύναμιν', 'NOUN', 'δύναμις'), (',', 'PUNCT', ','), ('καθάπερ', 'SCONJ', 'καθά'), ('ὑπὸ', 'ADP', 'ὑπό'), ('τὴν', 'DET', 'ὁ'), ('ἱππικὴν', 'ADJ', 'ἱππικός'), ('χαλινοποιικὴ', 'NOUN', 'χαλινοποιική'), ('καὶ', 'CCONJ', 'καί'), ('ὅσαι', 'PRON', 'ὅσος'), ('ἄλλαι', 'DET', 'ἄλλος'), ('τῶν', 'DET', 'ὁ'), ('ἱππικῶν', 'ADJ', 'ἱππικός'), ('ὀργάνων', 'NOUN', 'ὄργανον'), ('εἰσίν', 'VERB', 'εἰμί'), (',', 'PUNCT', ','), ('αὕτη', 'PRON', 'αὐτός'), ('δὲ', 'CCONJ', 'δέ'), ('καὶ', 'ADV', 'καί'), ('πᾶσα', 'DET', 'πᾶς'), ('πολεμικὴ', 'ADJ', 'πολεμικός'), ('πρᾶξις', 'NOUN', 'πρᾶξις'), ('ὑπὸ', 'ADP', 'ὑπό'), ('τὴν', 'DET', 'ὁ'), ('στρατηγικήν', 'ADJ', 'στρατηγικός'), (',', 'PUNCT', ','), ('κατὰ', 'ADP', 'κατά'), ('τὸν', 'DET', 'ὁ'), ('αὐτὸν', 'ADJ', 'αὐτός'), ('δὴ', 'ADV', 'δή'), ('τρόπον', 'NOUN', 'τρόπος'), ('ἄλλαι', 'ADJ', 'ἄλλος'), ('ὑφʼ', 'ADP', 'ὑπό'), ('ἑτέρας', 'ADJ', 'ἕτερος'), ('·', 'PUNCT', '·')]\n",
      "[('ἐν', 'ADP', 'ἐν'), ('ἁπάσαις', 'ADJ', 'ἅπας'), ('δὲ', 'ADV', 'δέ'), ('τὰ', 'DET', 'ὁ'), ('τῶν', 'DET', 'ὁ'), ('ἀρχιτεκτονικῶν', 'ADJ', 'ἀρχιτεκτονικῶν'), ('τέλη', 'NOUN', 'τέλος'), ('πάντων', 'DET', 'πᾶς'), ('ἐστὶν', 'AUX', 'εἰμί'), ('αἱρετώτερα', 'ADJ', 'αἱρετός'), ('τῶν', 'DET', 'ὁ'), ('ὑπʼ', 'ADP', 'ὑπό'), ('αὐτά', 'PRON', 'αὐτός'), ('·', 'PUNCT', '·')]\n",
      "[('τούτων', 'ADJ', 'οὗτος'), ('γὰρ', 'PART', 'γάρ'), ('χάριν', 'NOUN', 'χάρις'), ('κἀκεῖνα', 'ADJ', 'ἐκεῖνος'), ('διώκεται', 'VERB', 'διώκω'), ('.', 'PUNCT', '.')]\n"
     ]
    }
   ],
   "source": [
    "arist_test = apply_nlp(AGT[AGT[\"doc_id\"]==\"tlg0086.tlg010\"][\"sentences\"].tolist()[0])\n",
    "for doc in arist_test[:10]:\n",
    "    print([(token.text, token.pos_, token.lemma_) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
