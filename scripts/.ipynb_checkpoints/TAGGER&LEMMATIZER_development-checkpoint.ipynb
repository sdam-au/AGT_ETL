{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pyconll\n",
    "import json\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "import sddk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sciencedata.dk username (format '123456@au.dk'): 648597@au.dk\n",
      "sciencedata.dk password: ········\n",
      "connection with shared folder established with you as its owner\n",
      "endpoint variable has been configured to: https://sciencedata.dk/files/SDAM_root/\n"
     ]
    }
   ],
   "source": [
    "conf = sddk.configure(\"SDAM_root\", \"648597@au.dk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[38;5;2m✔ Successfully created model\u001b[0m\n",
      "134209it [00:06, 20598.20it/s]/word2vec_win2.txt\n",
      "\u001b[2K\u001b[38;5;2m✔ Loaded vectors from ../data/word2vec_win2.txt\u001b[0m\n",
      "\u001b[38;5;2m✔ Sucessfully compiled vocab\u001b[0m\n",
      "134382 entries, 134209 vectors\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy init-model xx ../models/spacy_grc_model_5_vectors --vectors-loc ../data/word2vec_win2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it into our envoronment\n",
    "nlp = spacy.load(\"../models/spacy_grc_model_5_vectors\")\n",
    "tagger = nlp.create_pipe(\"tagger\")\n",
    "nlp.add_pipe(tagger)\n",
    "optimizer = nlp.begin_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add lookup tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_lemma_lookup = json.load(open(\"../data/ag_lemma_lookup.json\"))\n",
    "ag_lemma_lookup_merged = json.load(open(\"../data/ag_lemma_lookup_merged.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ἐνέργεια'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag_lemma_lookup[\"NOUN\"][\"ἐνεργειῶν\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, this works but there is a substantial number of cases in which there is a wrong tag, e.g. \"VERB\" - in this case the lemmatization is unsuccessful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ἐνέργεια'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag_lemma_lookup_merged[\"ἐνεργειῶν\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nlp.vocab.lookups.remove_table(\"lemma_lookup\")\n",
    "    nlp.vocab.lookups.remove_table(\"lemma_lookup_merged\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let add our table to our model\n",
    "table = nlp.vocab.lookups.add_table(\"lemma_lookup\", ag_lemma_lookup)\n",
    "table = nlp.vocab.lookups.add_table(\"lemma_lookup_merged\", ag_lemma_lookup_merged)\n",
    "# nlp.vocab.lookups.remove_table(\"lemma_exc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'μιμνήσκω'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load it back from the model (to check functionality for future usage)\n",
    "ag_lemma_lookup = nlp.vocab.lookups.get_table(\"lemma_lookup\")\n",
    "ag_lemma_lookup_merged = nlp.vocab.lookups.get_table(\"lemma_lookup_merged\")\n",
    "\n",
    "\n",
    "ag_lemma_lookup[\"VERB\"][\"μνήσομαι\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install greek-accentuation\n",
    "from greek_accentuation.characters import strip_accents\n",
    "from greek_accentuation.syllabify import *\n",
    "from greek_accentuation.accentuation import *\n",
    "\n",
    "\n",
    "\n",
    "def grave_to_acute(string):\n",
    "    GRAVE = \"\\u0300\"\n",
    "    ACUTE = \"\\u0301\"\n",
    "    return unicodedata.normalize(\"NFC\", \"\".join(unicodedata.normalize(\"NFD\", string).replace(GRAVE, ACUTE)))\n",
    "\n",
    "\n",
    "def list_of_possible_accentuations(morph):\n",
    "    try:\n",
    "        if isinstance(morph, str):\n",
    "            morph = strip_accents(morph)\n",
    "            morph = rebreath(morph.lower())\n",
    "            s = syllabify(morph)\n",
    "            morph_vars = []\n",
    "            for accentuation in possible_accentuations(s, default_short=True):\n",
    "                pos, accent = accentuation #add_accentuation(s, accentuation))\n",
    "                final = s[1 - pos:] if pos > 1 else [\"\"]\n",
    "                morph_acc_var = \"\".join(s[:-pos] + [syllable_add_accent(s[-pos], accent)] + final)\n",
    "                morph_vars.append(morph_acc_var)\n",
    "                morph_vars.append(morph_acc_var.capitalize())\n",
    "            return morph_vars\n",
    "        else:\n",
    "            return []\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "def lemmatizer_v0(token_text, tag, old_lemma=None):\n",
    "    if (old_lemma==None) or (token_text == old_lemma):\n",
    "        lemma = token_text # start with assigning the word as it stands\n",
    "        try:\n",
    "            lemma = ag_lemma_lookup[tag][token_text]\n",
    "        except: \n",
    "            try:\n",
    "                lemma = ag_lemma_lookup[tag][grave_to_acute(token_text)]\n",
    "            except:\n",
    "                morph_vars = list_of_possible_accentuations(token_text)\n",
    "                for var in morph_vars:\n",
    "                    lemma = morph_vars[0]\n",
    "                    try:\n",
    "                        lemma = ag_lemma_lookup[tag][var] \n",
    "                        break\n",
    "                    except: \n",
    "                        pass\n",
    "        return lemma\n",
    "    else:\n",
    "        return old_lemma\n",
    "\n",
    "def update_lemmata(doc):\n",
    "    for token in doc:\n",
    "        token.lemma_ = lemmatizer(token.text, token.pos_, token.lemma_)\n",
    "    return doc   \n",
    "\n",
    "def apply_nlp(sentences_list):\n",
    "    spacy_docs = []\n",
    "    for sentence in sentences_list:\n",
    "        doc = nlp(sentence)\n",
    "        doc = update_lemmata(doc)\n",
    "        spacy_docs.append(doc)\n",
    "    return spacy_docs\n",
    "    \n",
    "def lemmatizer(token_text, tag, old_lemma=None):\n",
    "    if (old_lemma==None) or (token_text == old_lemma):\n",
    "        lemma = token_text # start with assigning the word as it stands\n",
    "        try:\n",
    "            lemma = ag_lemma_lookup[tag][token_text]\n",
    "        except:\n",
    "            try:\n",
    "                lemma = ag_lemma_lookup_merged[token_text]\n",
    "            except:\n",
    "                try:\n",
    "                    lemma = ag_lemma_lookup[tag][grave_to_acute(token_text)]\n",
    "                except:\n",
    "                    try:\n",
    "                        lemma = ag_lemma_lookup_merged[grave_to_acute(token_text)]\n",
    "                    except:\n",
    "                        morph_vars = list_of_possible_accentuations(token_text)\n",
    "                        for var in morph_vars:\n",
    "                            lemma = morph_vars[0]\n",
    "                            try:\n",
    "                                lemma = ag_lemma_lookup[tag][var] \n",
    "                                break\n",
    "                            except:\n",
    "                                try:\n",
    "                                    lemma = ag_lemma_lookup_merged[var]\n",
    "                                    break\n",
    "                                except:\n",
    "                                    pass\n",
    "        return lemma\n",
    "    else:\n",
    "        return old_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Παῦλος'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer(\"ΠΑΥΛΟΣ\", \"NOUN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'βίβλος'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer(\"ΒΙΒΛΟΣ\", \"NOUN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ἰησοῦς'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer(\"Ἰησοῦ\", \"NOUN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'βιββιβλός'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nonexistent word\n",
    "lemmatizer(\"ΒΙΒΒΙΒΛΟΣ\", \"NOUN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ἐνέργεια'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it works even with wrong POS-tag\n",
    "lemmatizer(\"ἐνεργειῶν\", \"VERB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training our own model with gold-parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Ancient_Greek-Perseus/master/grc_perseus-ud-train.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Ancient_Greek-Perseus/master/grc_perseus-ud-dev.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Ancient_Greek-PROIEL/master/grc_proiel-ud-train.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Ancient_Greek-PROIEL/master/grc_proiel-ud-dev.conllu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagged_data_from_url(url):\n",
    "    corpus = pyconll.load.iter_from_url(url)\n",
    "    tagged_data = []\n",
    "    for sentence in corpus:\n",
    "        words, tags, lemmata = [], [], []\n",
    "        for token in sentence:\n",
    "            words.append(token.form)\n",
    "            tags.append(token.upos)\n",
    "            lemmata.append(token.lemma)\n",
    "        if \"-dev.\" in url:\n",
    "            tagged_data.append((sentence.text, {\"words\" : words, \"tags\" : tags, \"lemmata\" : lemmata}))\n",
    "        else:\n",
    "            tagged_data.append((sentence.text, {\"words\" : words, \"tags\" : tags}))\n",
    "    print(\"File name: {0}; number of sentences: {1}\".format(url.rpartition(\"/\")[2], len(tagged_data)))\n",
    "    return tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name: grc_perseus-ud-train.conllu; number of sentences: 11476\n",
      "File name: grc_perseus-ud-dev.conllu; number of sentences: 1137\n",
      "File name: grc_proiel-ud-train.conllu; number of sentences: 15014\n",
      "File name: grc_proiel-ud-dev.conllu; number of sentences: 1019\n"
     ]
    }
   ],
   "source": [
    "perseus_train = tagged_data_from_url(urls[0])\n",
    "perseus_dev = tagged_data_from_url(urls[1])\n",
    "proiel_train = tagged_data_from_url(urls[2])\n",
    "proiel_dev = tagged_data_from_url(urls[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ἐρᾷ μὲν ἁγνὸς οὐρανὸς τρῶσαι χθόνα, ἔρως δὲ γαῖαν λαμβάνει γάμου τυχεῖν·', {'words': ['ἐρᾷ', 'μὲν', 'ἁγνὸς', 'οὐρανὸς', 'τρῶσαι', 'χθόνα', ',', 'ἔρως', 'δὲ', 'γαῖαν', 'λαμβάνει', 'γάμου', 'τυχεῖν', '·'], 'tags': ['VERB', 'ADV', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'PUNCT', 'NOUN', 'CCONJ', 'NOUN', 'VERB', 'NOUN', 'VERB', 'PUNCT']})\n"
     ]
    }
   ],
   "source": [
    "print(perseus_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = perseus_train + proiel_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26490"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens in proiel_dev: 13652\n",
      "tokens in perseus_dev: 22135\n"
     ]
    }
   ],
   "source": [
    "proiel_dev_len = sum([len(sent[1][\"tags\"]) for sent in proiel_dev])\n",
    "print(\"tokens in proiel_dev: \" + str(proiel_dev_len))\n",
    "\n",
    "perseus_dev_len = sum([len(sent[1][\"tags\"]) for sent in perseus_dev])\n",
    "print(\"tokens in perseus_dev: \" + str(perseus_dev_len))\n",
    "total_len = proiel_dev_len + perseus_dev_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Losses': {'tagger': 51263.093717992306}, 'proiel_tags_correct%': 88.17755640199239, 'proiel_lemmata_correct%': 88.0457075886317, 'perseus_tags_correct%': 84.85656200587304, 'perseus_lemmata_correct%': 88.11836458098034}\n",
      "{'Losses': {'tagger': 49777.75423413515}, 'proiel_tags_correct%': 88.22150600644594, 'proiel_lemmata_correct%': 88.06768239085848, 'perseus_tags_correct%': 85.14117912807771, 'perseus_lemmata_correct%': 88.17257736616219}\n",
      "{'Losses': {'tagger': 48755.168744176626}, 'proiel_tags_correct%': 88.1629065338412, 'proiel_lemmata_correct%': 88.0457075886317, 'perseus_tags_correct%': 84.74361870341089, 'perseus_lemmata_correct%': 88.14547097357128}\n",
      "{'Losses': {'tagger': 47687.73197457008}, 'proiel_tags_correct%': 88.27278054497509, 'proiel_lemmata_correct%': 88.03838265455612, 'perseus_tags_correct%': 85.05534221820646, 'perseus_lemmata_correct%': 88.15450643776825}\n",
      "{'Losses': {'tagger': 47103.34317180514}, 'proiel_tags_correct%': 88.00908291825374, 'proiel_lemmata_correct%': 88.06768239085848, 'perseus_tags_correct%': 85.52970408854755, 'perseus_lemmata_correct%': 88.2719674723289}\n",
      "{'Losses': {'tagger': 46579.16768336296}, 'proiel_tags_correct%': 88.17023146791679, 'proiel_lemmata_correct%': 88.0603574567829, 'perseus_tags_correct%': 85.75107296137338, 'perseus_lemmata_correct%': 88.28552066862436}\n",
      "{'Losses': {'tagger': 46038.51335296035}, 'proiel_tags_correct%': 88.4998535013185, 'proiel_lemmata_correct%': 88.08965719308526, 'perseus_tags_correct%': 85.19990964535803, 'perseus_lemmata_correct%': 88.30810932911677}\n",
      "{'Losses': {'tagger': 45384.21848958731}, 'proiel_tags_correct%': 88.47055376501612, 'proiel_lemmata_correct%': 88.0603574567829, 'perseus_tags_correct%': 85.1050372712898, 'perseus_lemmata_correct%': 88.22679015134402}\n",
      "{'Losses': {'tagger': 44830.61109781265}, 'proiel_tags_correct%': 88.25813067682391, 'proiel_lemmata_correct%': 88.0603574567829, 'perseus_tags_correct%': 85.94081770950983, 'perseus_lemmata_correct%': 88.2719674723289}\n",
      "{'Losses': {'tagger': 44512.60108632594}, 'proiel_tags_correct%': 88.49252856724289, 'proiel_lemmata_correct%': 88.0603574567829, 'perseus_tags_correct%': 85.42127851818387, 'perseus_lemmata_correct%': 88.19968375875311}\n"
     ]
    }
   ],
   "source": [
    "n_iter = 10\n",
    "\n",
    "from spacy.util import decaying\n",
    "dropout = decaying(0.6, 0.2, 1e-4)\n",
    "\n",
    "model_tests = []\n",
    "for i in range(n_iter):\n",
    "    \n",
    "    ### TRAIN THE MODEL\n",
    "    random.shuffle(train_data)\n",
    "    losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(texts, annotations, drop=0.6, sgd=optimizer, losses=losses)\n",
    "    \n",
    "    ### TEST THE MODEL\n",
    "    model_test = {}\n",
    "    model_test[\"Losses\"] = losses\n",
    "    proiel_tags_correct = 0\n",
    "    proiel_lemmata_correct = 0\n",
    "    for sent in proiel_dev:\n",
    "        doc = nlp(sent[0])\n",
    "        predicted_tags = [token.pos_ for token in doc]\n",
    "        predicted_lemmata = [lemmatizer(token.text, token.pos_) for token in doc]\n",
    "        given_tags = sent[1][\"tags\"]\n",
    "        given_lemmata = sent[1][\"lemmata\"]\n",
    "        proiel_tags_correct += len([p for p, g in zip(predicted_tags, given_tags) if p ==g])\n",
    "        proiel_lemmata_correct += len([p for p, g in zip(predicted_lemmata, given_lemmata) if p ==g])\n",
    "    model_test[\"proiel_tags_correct%\"] = (proiel_tags_correct / proiel_dev_len) * 100\n",
    "    model_test[\"proiel_lemmata_correct%\"] = (proiel_lemmata_correct / proiel_dev_len) * 100\n",
    "\n",
    "    perseus_tags_correct = 0\n",
    "    perseus_lemmata_correct = 0\n",
    "    for sent in perseus_dev:\n",
    "        doc = nlp(sent[0])\n",
    "        predicted_tags = [token.pos_ for token in doc]\n",
    "        predicted_lemmata = [lemmatizer(token.text, token.pos_) for token in doc]\n",
    "        given_tags = sent[1][\"tags\"]\n",
    "        given_lemmata = sent[1][\"lemmata\"]\n",
    "        perseus_tags_correct += len([p for p, g in zip(predicted_tags, given_tags) if p ==g])\n",
    "        perseus_lemmata_correct += len([p for p, g in zip(predicted_lemmata, given_lemmata) if p ==g])\n",
    "    model_test[\"perseus_tags_correct%\"] = (perseus_tags_correct / perseus_dev_len) * 100\n",
    "    model_test[\"perseus_lemmata_correct%\"] = (perseus_lemmata_correct / perseus_dev_len) * 100\n",
    "    print(model_test)\n",
    "    model_tests.append(model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(\"../models/spacy_grc_model_5_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'proiel_tags_correct%': 87.90653384119544,\n",
       " 'proiel_lemmata_correct%': 88.13360679753882,\n",
       " 'perseus_tags_correct%': 85.05534221820646,\n",
       " 'perseus_lemmata_correct%': 88.1364355093743}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMPARE WITH OUR FIRST MODEL\n",
    "nlp_1 = spacy.load(\"../models/spacy_grc_model_1\")\n",
    "\n",
    "model_test = {}\n",
    "proiel_tags_correct = 0\n",
    "proiel_lemmata_correct = 0\n",
    "for sent in proiel_dev:\n",
    "    doc = nlp_1(sent[0])\n",
    "    predicted_tags = [token.pos_ for token in doc]\n",
    "    predicted_lemmata = [lemmatizer(token.text, token.pos_) for token in doc]\n",
    "    given_tags = sent[1][\"tags\"]\n",
    "    given_lemmata = sent[1][\"lemmata\"]\n",
    "    proiel_tags_correct += len([p for p, g in zip(predicted_tags, given_tags) if p ==g])\n",
    "    proiel_lemmata_correct += len([p for p, g in zip(predicted_lemmata, given_lemmata) if p ==g])\n",
    "model_test[\"proiel_tags_correct%\"] = (proiel_tags_correct / proiel_dev_len) * 100\n",
    "model_test[\"proiel_lemmata_correct%\"] = (proiel_lemmata_correct / proiel_dev_len) * 100\n",
    "\n",
    "perseus_tags_correct = 0\n",
    "perseus_lemmata_correct = 0\n",
    "for sent in perseus_dev:\n",
    "    doc = nlp_1(sent[0])\n",
    "    predicted_tags = [token.pos_ for token in doc]\n",
    "    predicted_lemmata = [lemmatizer(token.text, token.pos_) for token in doc]\n",
    "    given_tags = sent[1][\"tags\"]\n",
    "    given_lemmata = sent[1][\"lemmata\"]\n",
    "    perseus_tags_correct += len([p for p, g in zip(predicted_tags, given_tags) if p ==g])\n",
    "    perseus_lemmata_correct += len([p for p, g in zip(predicted_lemmata, given_lemmata) if p ==g])\n",
    "model_test[\"perseus_tags_correct%\"] = (perseus_tags_correct / perseus_dev_len) * 100\n",
    "model_test[\"perseus_lemmata_correct%\"] = (perseus_lemmata_correct / perseus_dev_len) * 100\n",
    "\n",
    "model_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our results with first model:\n",
    "```\n",
    "{'proiel_tags_correct%': 87.90653384119544,\n",
    " 'proiel_lemmata_correct%': 85.62115440961031,\n",
    " 'perseus_tags_correct%': 85.05534221820646,\n",
    " 'perseus_lemmata_correct%': 82.66997967020556}\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGT = sddk.read_file(\"SDAM_data/AGT/AGT_preprocessed_20201127.json\", \"df\", conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ΠΑΥΛΟΣ', 'VERB', 'Παῦλος'), ('ἀπόστολος', 'NOUN', 'ἀπόστολος'), (',', 'PUNCT', ','), ('οὐκ', 'ADV', 'οὐ'), ('ἀπʼ', 'ADP', 'ἄπʼ'), ('ἀνθρώπων', 'NOUN', 'ἄνθρωπος'), ('οὐδὲ', 'CCONJ', 'οὐδέ'), ('διʼ', 'ADP', 'δίʼ'), ('ἀνθρώπου', 'NOUN', 'ἄνθρωπος'), ('ἀλλὰ', 'CCONJ', 'ἀλλά'), ('διὰ', 'ADP', 'διά'), ('Ἰησοῦ', 'PROPN', 'Ἰησοῦς'), ('Χριστοῦ', 'PROPN', 'Χριστός'), ('καὶ', 'CCONJ', 'καί'), ('θεοῦ', 'NOUN', 'θεός'), ('πατρὸς', 'NOUN', 'πατήρ'), ('τοῦ', 'DET', 'ὁ'), ('ἐγείραντος', 'VERB', 'ἐγείρω'), ('αὐτὸν', 'PRON', 'αὐτός'), ('ἐκ', 'ADP', 'ἐκ'), ('νεκρῶν', 'ADJ', 'νεκρός'), (',', 'PUNCT', ','), ('καὶ', 'CCONJ', 'καί'), ('οἱ', 'DET', 'ὁ'), ('σὺν', 'ADP', 'σύν'), ('ἐμοὶ', 'PRON', 'ἐγώ'), ('πάντες', 'ADJ', 'πᾶς'), ('ἀδελφοί', 'NOUN', 'ἀδελφός'), (',', 'PUNCT', ','), ('ταῖς', 'DET', 'ὁ'), ('ἐκκλησίαις', 'NOUN', 'ἐκκλησία'), ('τῆς', 'DET', 'ὁ'), ('Γαλατίας', 'NOUN', 'Γαλατία'), ('·', 'PUNCT', '·')]\n"
     ]
    }
   ],
   "source": [
    "doc = update_lemmata(nlp(AGT[AGT[\"author_id\"]==\"tlg0031paul\"].iloc[3][\"sentences\"][0]))\n",
    "print([(token.text, token.pos_, token.lemma_) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('πᾶσα', 'DET', 'πᾶς'), ('τέχνη', 'NOUN', 'τέχνη'), ('καὶ', 'CCONJ', 'καί'), ('πᾶσα', 'DET', 'πᾶς'), ('μέθοδος', 'NOUN', 'μέθοδος'), (',', 'PUNCT', ','), ('ὁμοίως', 'ADV', 'ὁμοίως'), ('δὲ', 'PART', 'δέ'), ('πρᾶξίς', 'ADJ', 'πρᾶξις'), ('τε', 'PART', 'τε'), ('καὶ', 'CCONJ', 'καί'), ('προαίρεσις', 'NOUN', 'προαίρεσις'), (',', 'PUNCT', ','), ('ἀγαθοῦ', 'ADJ', 'ἀγαθός'), ('τινὸς', 'PRON', 'τίς'), ('ἐφίεσθαι', 'VERB', 'ἐφίημι'), ('δοκεῖ', 'VERB', 'δοκέω'), ('·', 'PUNCT', '·')]\n",
      "[('διὸ', 'ADV', 'διό'), ('καλῶς', 'ADV', 'καλῶς'), ('ἀπεφήναντο', 'VERB', 'ἀποφαίνω'), ('τἀγαθόν', 'NOUN', 'ἀγαθός'), (',', 'PUNCT', ','), ('οὗ', 'PRON', 'ὅς'), ('πάντʼ', 'ADJ', 'πάντʼ'), ('ἐφίεται', 'VERB', 'ἐφίημι'), ('.', 'PUNCT', '.')]\n",
      "[('διαφορὰ', 'NOUN', 'διαφορά'), ('δέ', 'ADV', 'δέ'), ('τις', 'ADJ', 'τὶς'), ('φαίνεται', 'VERB', 'φαίνω'), ('τῶν', 'DET', 'ὁ'), ('τελῶν', 'NOUN', 'τέλος'), ('·', 'PUNCT', '·')]\n",
      "[('τὰ', 'DET', 'ὁ'), ('μὲν', 'PART', 'μέν'), ('γάρ', 'PART', 'γάρ'), ('εἰσιν', 'VERB', 'εἰμί'), ('ἐνέργειαι', 'NOUN', 'ἐνέργεια'), (',', 'PUNCT', ','), ('τὰ', 'DET', 'ὁ'), ('δὲ', 'CCONJ', 'δέ'), ('παρʼ', 'ADP', 'πάρʼ'), ('αὐτὰς', 'PRON', 'αὐτός'), ('ἔργα', 'NOUN', 'ἔργον'), ('τινά', 'PRON', 'τις'), ('.', 'PUNCT', '.')]\n",
      "[('ὧν', 'PRON', 'ὅς'), ('δʼ', 'ADV', 'δʼ'), ('εἰσὶ', 'AUX', 'εἰμί'), ('τέλη', 'NOUN', 'τέλος'), ('τινὰ', 'DET', 'τὶς'), ('παρὰ', 'ADP', 'παρά'), ('τὰς', 'DET', 'ὁ'), ('πράξεις', 'NOUN', 'πρᾶξις'), (',', 'PUNCT', ','), ('ἐν', 'ADP', 'ἐν'), ('τούτοις', 'PRON', 'οὗτος'), ('βελτίω', 'ADJ', 'βελτίων'), ('πέφυκε', 'VERB', 'φύω'), ('τῶν', 'DET', 'ὁ'), ('ἐνεργειῶν', 'ADJ', 'ἐνέργεια'), ('τὰ', 'DET', 'ὁ'), ('ἔργα', 'NOUN', 'ἔργον'), ('.', 'PUNCT', '.')]\n",
      "[('πολλῶν', 'ADJ', 'πολύς'), ('δὲ', 'ADV', 'δέ'), ('πράξεων', 'NOUN', 'πρᾶξις'), ('οὐσῶν', 'AUX', 'εἰμί'), ('καὶ', 'CCONJ', 'καί'), ('τεχνῶν', 'NOUN', 'τέχνη'), ('καὶ', 'CCONJ', 'καί'), ('ἐπιστημῶν', 'NOUN', 'ἐπιστήμη'), ('πολλὰ', 'ADJ', 'πολύς'), ('γίνεται', 'VERB', 'γίγνομαι'), ('καὶ', 'CCONJ', 'καί'), ('τὰ', 'DET', 'ὁ'), ('τέλη', 'NOUN', 'τέλος'), ('·', 'PUNCT', '·')]\n",
      "[('ἰατρικῆς', 'NOUN', 'ἰατρικός'), ('μὲν', 'PART', 'μέν'), ('γὰρ', 'PART', 'γάρ'), ('ὑγίεια', 'NOUN', 'ὑγίεια'), (',', 'PUNCT', ','), ('ναυπηγικῆς', 'VERB', 'ναυπηγικός'), ('δὲ', 'PART', 'δέ'), ('πλοῖον', 'NOUN', 'πλοῖον'), (',', 'PUNCT', ','), ('στρατηγικῆς', 'ADJ', 'στρατηγικός'), ('δὲ', 'PART', 'δέ'), ('νίκη', 'NOUN', 'νίκη'), (',', 'PUNCT', ','), ('οἰκονομικῆς', 'ADJ', 'οἰκονομικός'), ('δὲ', 'PART', 'δέ'), ('πλοῦτος', 'NOUN', 'πλοῦτος'), ('.', 'PUNCT', '.')]\n",
      "[('ὅσαι', 'ADJ', 'ὅσος'), ('δʼ', 'ADV', 'δʼ'), ('εἰσὶ', 'AUX', 'εἰμί'), ('τῶν', 'DET', 'ὁ'), ('τοιούτων', 'ADJ', 'τοιοῦτος'), ('ὑπὸ', 'ADP', 'ὑπό'), ('μίαν', 'NUM', 'εἷς'), ('τινὰ', 'DET', 'τὶς'), ('δύναμιν', 'NOUN', 'δύναμις'), (',', 'PUNCT', ','), ('καθάπερ', 'SCONJ', 'καθά'), ('ὑπὸ', 'ADP', 'ὑπό'), ('τὴν', 'DET', 'ὁ'), ('ἱππικὴν', 'NOUN', 'ἱππικός'), ('χαλινοποιικὴ', 'NOUN', 'χαλινοποιική'), ('καὶ', 'CCONJ', 'καί'), ('ὅσαι', 'ADJ', 'ὅσος'), ('ἄλλαι', 'DET', 'ἄλλος'), ('τῶν', 'DET', 'ὁ'), ('ἱππικῶν', 'ADJ', 'ἱππικός'), ('ὀργάνων', 'NOUN', 'ὄργανον'), ('εἰσίν', 'VERB', 'εἰμί'), (',', 'PUNCT', ','), ('αὕτη', 'ADJ', 'οὗτος'), ('δὲ', 'CCONJ', 'δέ'), ('καὶ', 'ADV', 'καί'), ('πᾶσα', 'DET', 'πᾶς'), ('πολεμικὴ', 'ADJ', 'πολεμικός'), ('πρᾶξις', 'NOUN', 'πρᾶξις'), ('ὑπὸ', 'ADP', 'ὑπό'), ('τὴν', 'DET', 'ὁ'), ('στρατηγικήν', 'NOUN', 'στρατηγικός'), (',', 'PUNCT', ','), ('κατὰ', 'ADP', 'κατά'), ('τὸν', 'DET', 'ὁ'), ('αὐτὸν', 'PRON', 'αὐτός'), ('δὴ', 'ADV', 'δή'), ('τρόπον', 'NOUN', 'τρόπος'), ('ἄλλαι', 'ADJ', 'ἄλλος'), ('ὑφʼ', 'ADP', 'ὕφʼ'), ('ἑτέρας', 'ADJ', 'ἕτερος'), ('·', 'PUNCT', '·')]\n",
      "[('ἐν', 'ADP', 'ἐν'), ('ἁπάσαις', 'ADJ', 'ἅπας'), ('δὲ', 'ADV', 'δέ'), ('τὰ', 'DET', 'ὁ'), ('τῶν', 'DET', 'ὁ'), ('ἀρχιτεκτονικῶν', 'VERB', 'ἀρχιτεκτονικών'), ('τέλη', 'NOUN', 'τέλος'), ('πάντων', 'DET', 'πᾶς'), ('ἐστὶν', 'AUX', 'εἰμί'), ('αἱρετώτερα', 'ADJ', 'αἱρετός'), ('τῶν', 'DET', 'ὁ'), ('ὑπʼ', 'ADP', 'ὕπʼ'), ('αὐτά', 'PRON', 'αὐτός'), ('·', 'PUNCT', '·')]\n",
      "[('τούτων', 'ADJ', 'οὗτος'), ('γὰρ', 'PART', 'γάρ'), ('χάριν', 'NOUN', 'χάρις'), ('κἀκεῖνα', 'ADJ', 'ἐκεῖνος'), ('διώκεται', 'VERB', 'διώκω'), ('.', 'PUNCT', '.')]\n"
     ]
    }
   ],
   "source": [
    "arist_test = apply_nlp(AGT[AGT[\"doc_id\"]==\"tlg0086.tlg010\"][\"sentences\"].tolist()[0])\n",
    "for doc in arist_test[:10]:\n",
    "    print([(token.text, token.pos_, token.lemma_) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
