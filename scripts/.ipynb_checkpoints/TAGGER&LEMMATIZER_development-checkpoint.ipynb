{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pyconll\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "import sddk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sciencedata.dk username (format '123456@au.dk'): 648597@au.dk\n",
      "sciencedata.dk password: ········\n",
      "connection with shared folder established with you as its owner\n",
      "endpoint variable has been configured to: https://sciencedata.dk/files/SDAM_root/\n"
     ]
    }
   ],
   "source": [
    "conf = sddk.configure(\"SDAM_root\", \"648597@au.dk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[38;5;2m✔ Successfully created model\u001b[0m\n",
      "134209it [00:06, 20760.69it/s]/word2vec_win2.txt\n",
      "\u001b[2K\u001b[38;5;2m✔ Loaded vectors from ../data/word2vec_win2.txt\u001b[0m\n",
      "\u001b[38;5;2m✔ Sucessfully compiled vocab\u001b[0m\n",
      "134382 entries, 134209 vectors\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy init-model xx ../models/spacy_grc_model_6_vectors --vectors-loc ../data/word2vec_win2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kasev/.local/lib/python3.6/site-packages/spacy/language.py:639: UserWarning: [W022] Training a new part-of-speech tagger using a model with no lemmatization rules or data. This means that the trained model may not be able to lemmatize correctly. If this is intentional or the language you're using doesn't have lemmatization data, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed.\n",
      "  **kwargs\n",
      "/home/kasev/.local/lib/python3.6/site-packages/spacy/language.py:639: UserWarning: [W033] Training a new part-of-speech tagger using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed. The languages with lexeme normalization tables are currently: da, de, el, en, id, lb, pt, ru, sr, ta, th.\n",
      "  **kwargs\n"
     ]
    }
   ],
   "source": [
    "# load it into our envoronment\n",
    "nlp = spacy.load(\"../models/spacy_grc_model_6_vectors\")\n",
    "tagger = nlp.create_pipe(\"tagger\")\n",
    "nlp.add_pipe(tagger)\n",
    "optimizer = nlp.begin_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add lookup tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_lemma_lookup = json.load(open(\"../data/ag_lemma_lookup.json\"))\n",
    "ag_lemma_lookup_merged = json.load(open(\"../data/ag_lemma_lookup_merged.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ἐνέργεια'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag_lemma_lookup[\"NOUN\"][\"ἐνεργειῶν\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, this works but there is a substantial number of cases in which there is a wrong tag, e.g. \"VERB\" - in this case the lemmatization is unsuccessful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ἐνέργεια'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag_lemma_lookup_merged[\"ἐνεργειῶν\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nlp.vocab.lookups.remove_table(\"lemma_lookup\")\n",
    "    nlp.vocab.lookups.remove_table(\"lemma_lookup_merged\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-cbf5085d2c1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# let add our table to our model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookups\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lemma_lookup\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_lemma_lookup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookups\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lemma_lookup_merged\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_lemma_lookup_merged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# nlp.vocab.lookups.remove_table(\"lemma_exc\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "# let add our table to our model\n",
    "table = nlp.vocab.lookups.add_table(\"lemma_lookup\", ag_lemma_lookup)\n",
    "table = nlp.vocab.lookups.add_table(\"lemma_lookup_merged\", ag_lemma_lookup_merged)\n",
    "# nlp.vocab.lookups.remove_table(\"lemma_exc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-cdb2e9fd7f8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load it back from the model (to check functionality for future usage)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mag_lemma_lookup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookups\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lemma_lookup\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mag_lemma_lookup_merged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookups\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lemma_lookup_merged\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "# load it back from the model (to check functionality for future usage)\n",
    "ag_lemma_lookup = nlp.vocab.lookups.get_table(\"lemma_lookup\")\n",
    "ag_lemma_lookup_merged = nlp.vocab.lookups.get_table(\"lemma_lookup_merged\")\n",
    "\n",
    "\n",
    "ag_lemma_lookup[\"VERB\"][\"μνήσομαι\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install greek-accentuation\n",
    "from greek_accentuation.characters import strip_accents\n",
    "from greek_accentuation.syllabify import *\n",
    "from greek_accentuation.accentuation import *\n",
    "\n",
    "\n",
    "\n",
    "def grave_to_acute(string):\n",
    "    GRAVE = \"\\u0300\"\n",
    "    ACUTE = \"\\u0301\"\n",
    "    return unicodedata.normalize(\"NFC\", \"\".join(unicodedata.normalize(\"NFD\", string).replace(GRAVE, ACUTE)))\n",
    "\n",
    "\n",
    "def list_of_possible_accentuations(morph):\n",
    "    try:\n",
    "        if isinstance(morph, str):\n",
    "            morph = strip_accents(morph)\n",
    "            morph = rebreath(morph.lower())\n",
    "            s = syllabify(morph)\n",
    "            morph_vars = []\n",
    "            for accentuation in possible_accentuations(s, default_short=True):\n",
    "                pos, accent = accentuation #add_accentuation(s, accentuation))\n",
    "                final = s[1 - pos:] if pos > 1 else [\"\"]\n",
    "                morph_acc_var = \"\".join(s[:-pos] + [syllable_add_accent(s[-pos], accent)] + final)\n",
    "                morph_vars.append(morph_acc_var)\n",
    "                morph_vars.append(morph_acc_var.capitalize())\n",
    "            return morph_vars\n",
    "        else:\n",
    "            return []\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "def lemmatizer_v0(token_text, tag, old_lemma=None):\n",
    "    if (old_lemma==None) or (token_text == old_lemma):\n",
    "        lemma = token_text # start with assigning the word as it stands\n",
    "        try:\n",
    "            lemma = ag_lemma_lookup[tag][token_text]\n",
    "        except: \n",
    "            try:\n",
    "                lemma = ag_lemma_lookup[tag][grave_to_acute(token_text)]\n",
    "            except:\n",
    "                morph_vars = list_of_possible_accentuations(token_text)\n",
    "                for var in morph_vars:\n",
    "                    lemma = morph_vars[0]\n",
    "                    try:\n",
    "                        lemma = ag_lemma_lookup[tag][var] \n",
    "                        break\n",
    "                    except: \n",
    "                        pass\n",
    "        return lemma\n",
    "    else:\n",
    "        return old_lemma\n",
    "\n",
    "def grc_doc_lemmatizer(doc):\n",
    "    for token in doc:\n",
    "        token.lemma_, token.pos_ = lemmatizer(token.text, token.pos_, token.lemma_)\n",
    "    return doc   \n",
    "\n",
    "def apply_nlp(sentences_list):\n",
    "    spacy_docs = []\n",
    "    for sentence in sentences_list:\n",
    "        doc = nlp(sentence)\n",
    "        doc = update_lemmata(doc)\n",
    "        spacy_docs.append(doc)\n",
    "    return spacy_docs\n",
    "    \n",
    "def check_other_tags(token_text, tag):\n",
    "    lemma = token_text\n",
    "    match = False\n",
    "    # for new tags in specific order...\n",
    "    for new_tag in [\"PUNCT\",'ADP', 'AUX', 'DET', 'INTJ', 'PART', \"ADV\", \"ADJ\", \"NOUN\", \"PROPN\", \"VERB\", \"NUM\"]:\n",
    "        try:\n",
    "            lemma = ag_lemma_lookup[new_tag][token_text]\n",
    "            tag = new_tag\n",
    "            match = True\n",
    "            break\n",
    "        except:\n",
    "            pass\n",
    "    return lemma, tag, match\n",
    "    \n",
    "def lemmatizer(token_text, tag, old_lemma=None):\n",
    "    if (old_lemma==None) or (token_text == old_lemma):\n",
    "        lemma = token_text.lower() # start with assigning the word as it stands\n",
    "        try:\n",
    "            lemma = ag_lemma_lookup[tag][token_text]\n",
    "        except:\n",
    "            lemma, tag, match = check_other_tags(token_text, tag)\n",
    "            if match == False:\n",
    "                try:\n",
    "                    lemma = ag_lemma_lookup[tag][grave_to_acute(token_text)]\n",
    "                except:\n",
    "                    lemma, tag, match = check_other_tags(token_text, tag)\n",
    "                    if match == False:\n",
    "                        morph_vars = list_of_possible_accentuations(token_text)\n",
    "                        for var in morph_vars:\n",
    "                            try:\n",
    "                                lemma = ag_lemma_lookup[tag][var] \n",
    "                                break\n",
    "                            except:\n",
    "                                lemma, tag, match = check_other_tags(var, tag)\n",
    "                                if match ==True:\n",
    "                                    break\n",
    "        return lemma, tag\n",
    "    else:\n",
    "        return old_lemma, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('διά', 'ADP')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer(\"δι’\", \"DET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Παῦλος', 'NOUN')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer(\"ΠΑΥΛΟΣ\", \"VERB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('βίβλος', 'NOUN')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer(\"ΒΙΒΛΟΣ\", \"ADJ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Ἰησοῦς', 'NOUN')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer(\"Ἰησοῦ\", \"NOUN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Βίββιβλος', 'NOUN')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nonexistent word\n",
    "lemmatizer(\"ΒΙΒΒΙΒΛΟΣ\", \"NOUN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ἐνέργεια', 'NOUN')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it works even with wrong POS-tag\n",
    "lemmatizer(\"ἐνεργειῶν\", \"VERB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training our own model with gold-parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Ancient_Greek-Perseus/master/grc_perseus-ud-train.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Ancient_Greek-Perseus/master/grc_perseus-ud-dev.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Ancient_Greek-PROIEL/master/grc_proiel-ud-train.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Ancient_Greek-PROIEL/master/grc_proiel-ud-dev.conllu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_encoding(string):\n",
    "    return unicodedata.normalize(\"NFC\", string)\n",
    "v_ud = \"’\"\n",
    "v_agt = \"ʼ\"\n",
    "\n",
    "v_ud == v_agt # because of that, we will everywhere use the agt version, since it does not confuse the word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagged_data_from_url(url):\n",
    "    corpus = pyconll.load.iter_from_url(url)\n",
    "    tagged_data = []\n",
    "    for sentence in corpus:\n",
    "        words, tags, lemmata = [], [], []\n",
    "        for token in sentence:\n",
    "            words.append(normalize_encoding(re.sub(v_ud, v_agt, token.form)))\n",
    "            tags.append(token.upos)\n",
    "            lemmata.append(normalize_encoding(token.lemma))\n",
    "        if \"-dev.\" in url:\n",
    "            tagged_data.append((normalize_encoding(re.sub(v_ud, v_agt, sentence.text)), {\"words\" : words, \"tags\" : tags, \"lemmata\" : lemmata}))\n",
    "        else:\n",
    "            tagged_data.append((normalize_encoding(re.sub(v_ud, v_agt, sentence.text)), {\"words\" : words, \"tags\" : tags}))\n",
    "    print(\"File name: {0}; number of sentences: {1}\".format(url.rpartition(\"/\")[2], len(tagged_data)))\n",
    "    return tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name: grc_perseus-ud-train.conllu; number of sentences: 11476\n",
      "File name: grc_perseus-ud-dev.conllu; number of sentences: 1137\n",
      "File name: grc_proiel-ud-train.conllu; number of sentences: 15014\n",
      "File name: grc_proiel-ud-dev.conllu; number of sentences: 1019\n"
     ]
    }
   ],
   "source": [
    "perseus_train = tagged_data_from_url(urls[0])\n",
    "perseus_dev = tagged_data_from_url(urls[1])\n",
    "proiel_train = tagged_data_from_url(urls[2])\n",
    "proiel_dev = tagged_data_from_url(urls[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ἐρᾷ μὲν ἁγνὸς οὐρανὸς τρῶσαι χθόνα, ἔρως δὲ γαῖαν λαμβάνει γάμου τυχεῖν·', {'words': ['ἐρᾷ', 'μὲν', 'ἁγνὸς', 'οὐρανὸς', 'τρῶσαι', 'χθόνα', ',', 'ἔρως', 'δὲ', 'γαῖαν', 'λαμβάνει', 'γάμου', 'τυχεῖν', '·'], 'tags': ['VERB', 'ADV', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'PUNCT', 'NOUN', 'CCONJ', 'NOUN', 'VERB', 'NOUN', 'VERB', 'PUNCT']})\n"
     ]
    }
   ],
   "source": [
    "print(perseus_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = perseus_train + proiel_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26490"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens in proiel_dev: 13652\n",
      "tokens in perseus_dev: 22135\n"
     ]
    }
   ],
   "source": [
    "proiel_dev_len = sum([len(sent[1][\"tags\"]) for sent in proiel_dev])\n",
    "print(\"tokens in proiel_dev: \" + str(proiel_dev_len))\n",
    "\n",
    "perseus_dev_len = sum([len(sent[1][\"tags\"]) for sent in perseus_dev])\n",
    "print(\"tokens in perseus_dev: \" + str(perseus_dev_len))\n",
    "total_len = proiel_dev_len + perseus_dev_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tests = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_list = []\n",
    "\n",
    "n_iter = 1\n",
    "\n",
    "model_tests = []\n",
    "for i in range(n_iter):\n",
    "    \n",
    "    ### TRAIN THE MODEL\n",
    "    random.shuffle(train_data)\n",
    "    losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        #for token_text, tag in zip\n",
    "        annotations_list.append(annotations)\n",
    "        #nlp.update(texts, annotations, drop=0.3, sgd=optimizer, losses=losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_tags = []\n",
    "all_tags = 0\n",
    "corrected_tags_N = 0\n",
    "for annotations_entry in annotations_list:\n",
    "    all_tags = all_tags + len(annotations_entry[\"tags\"])\n",
    "    for token_text, tag in zip(annotations_entry[\"words\"], annotations_entry[\"tags\"]):\n",
    "        corrected_tag = lemmatizer(token_text, tag)[1]\n",
    "        if corrected_tag != tag:\n",
    "            corrected_tags_N = corrected_tags_N + 1\n",
    "            corrected_tags.append(corrected_tag)\n",
    "        else:\n",
    "            corrected_tags.append(tag)\n",
    "    annotations_entry[\"tags\"] = corrected_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9190122720"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(corrected_tags_N / all_tags, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Losses': {'tagger': 28335.17343378486}, 'proiel_tags_correct%': 95.32669205977146, 'proiel_lemmata_correct%': 91.49575153823616, 'perseus_tags_correct%': 87.07928619832845, 'perseus_lemmata_correct%': 89.26586853399593}\n",
      "{'Losses': {'tagger': 27338.618908771314}, 'proiel_tags_correct%': 95.34134192792266, 'proiel_lemmata_correct%': 91.5616759449165, 'perseus_tags_correct%': 86.92116557488141, 'perseus_lemmata_correct%': 89.28845719448837}\n",
      "{'Losses': {'tagger': 26388.92041359283}, 'proiel_tags_correct%': 95.44389100498096, 'proiel_lemmata_correct%': 91.58365074714328, 'perseus_tags_correct%': 86.86695278969957, 'perseus_lemmata_correct%': 89.41495369324599}\n",
      "{'Losses': {'tagger': 25767.658849308267}, 'proiel_tags_correct%': 95.64166422502197, 'proiel_lemmata_correct%': 91.5543510108409, 'perseus_tags_correct%': 87.01152021685115, 'perseus_lemmata_correct%': 89.36525864016264}\n",
      "{'Losses': {'tagger': 25209.311948458664}, 'proiel_tags_correct%': 95.51714034573689, 'proiel_lemmata_correct%': 91.51772634046293, 'perseus_tags_correct%': 87.567201264965, 'perseus_lemmata_correct%': 89.36074090806414}\n",
      "{'Losses': {'tagger': 24590.810606728075}, 'proiel_tags_correct%': 95.67096396132435, 'proiel_lemmata_correct%': 91.58365074714328, 'perseus_tags_correct%': 87.23288908967697, 'perseus_lemmata_correct%': 89.41495369324599}\n",
      "{'Losses': {'tagger': 23867.269306235044}, 'proiel_tags_correct%': 95.64898915909757, 'proiel_lemmata_correct%': 91.56900087899209, 'perseus_tags_correct%': 87.27354867856336, 'perseus_lemmata_correct%': 89.38332956855658}\n",
      "{'Losses': {'tagger': 23534.353230834007}, 'proiel_tags_correct%': 95.56841488426603, 'proiel_lemmata_correct%': 91.54702607676532, 'perseus_tags_correct%': 87.27806641066185, 'perseus_lemmata_correct%': 89.40140049695053}\n",
      "{'Losses': {'tagger': 23210.65416306723}, 'proiel_tags_correct%': 95.67828889539994, 'proiel_lemmata_correct%': 91.5616759449165, 'perseus_tags_correct%': 87.41811610571493, 'perseus_lemmata_correct%': 89.29297492658685}\n",
      "{'Losses': {'tagger': 22660.044650241733}, 'proiel_tags_correct%': 95.81013770876062, 'proiel_lemmata_correct%': 91.59097568121886, 'perseus_tags_correct%': 87.68918003162412, 'perseus_lemmata_correct%': 89.4104359611475}\n"
     ]
    }
   ],
   "source": [
    "n_iter = 10\n",
    "\n",
    "nlp.add_pipe(grc_doc_lemmatizer, \"grc_doc_lemmatizer\", after=\"tagger\") # grc_doc_lemmatizer\n",
    "\n",
    "model_tests = []\n",
    "for i in range(n_iter):\n",
    "    \n",
    "    ### TRAIN THE MODEL\n",
    "    random.shuffle(train_data)\n",
    "    losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(texts, annotations, drop=0.3, sgd=optimizer, losses=losses)\n",
    "    \n",
    "    ### TEST THE MODEL\n",
    "    model_test = {}\n",
    "    model_test[\"Losses\"] = losses\n",
    "    proiel_tags_correct = 0\n",
    "    proiel_lemmata_correct = 0\n",
    "    for sent in proiel_dev:\n",
    "        doc = nlp(sent[0])\n",
    "        predicted_tags = [token.pos_ for token in doc]\n",
    "        predicted_lemmata = [lemmatizer(token.text, token.pos_) for token in doc]\n",
    "        given_tags = sent[1][\"tags\"]\n",
    "        given_lemmata = sent[1][\"lemmata\"]\n",
    "        proiel_tags_correct += len([p for p, g in zip(predicted_tags, given_tags) if p ==g])\n",
    "        proiel_lemmata_correct += len([p for p, g in zip(predicted_lemmata, given_lemmata) if p ==g])\n",
    "    model_test[\"proiel_tags_correct%\"] = (proiel_tags_correct / proiel_dev_len) * 100\n",
    "    model_test[\"proiel_lemmata_correct%\"] = (proiel_lemmata_correct / proiel_dev_len) * 100\n",
    "\n",
    "    perseus_tags_correct = 0\n",
    "    perseus_lemmata_correct = 0\n",
    "    for sent in perseus_dev:\n",
    "        doc = nlp(sent[0])\n",
    "        predicted_tags = [token.pos_ for token in doc]\n",
    "        predicted_lemmata = [lemmatizer(token.text, token.pos_) for token in doc]\n",
    "        given_tags = sent[1][\"tags\"]\n",
    "        given_lemmata = sent[1][\"lemmata\"]\n",
    "        perseus_tags_correct += len([p for p, g in zip(predicted_tags, given_tags) if p ==g])\n",
    "        perseus_lemmata_correct += len([p for p, g in zip(predicted_lemmata, given_lemmata) if p ==g])\n",
    "    model_test[\"perseus_tags_correct%\"] = (perseus_tags_correct / perseus_dev_len) * 100\n",
    "    model_test[\"perseus_lemmata_correct%\"] = (perseus_lemmata_correct / perseus_dev_len) * 100\n",
    "    print(model_test)\n",
    "    model_tests.append(model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(\"../models/spacy_grc_model_6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'proiel_tags_correct%': 93.59068268385585,\n",
       " 'proiel_lemmata_correct%': 91.31995312042191,\n",
       " 'perseus_tags_correct%': 85.05534221820646,\n",
       " 'perseus_lemmata_correct%': 89.01739326857917}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMPARE WITH OUR FIRST MODEL\n",
    "nlp_1 = spacy.load(\"../models/spacy_grc_model_1\")\n",
    "\n",
    "model_test = {}\n",
    "proiel_tags_correct = 0\n",
    "proiel_lemmata_correct = 0\n",
    "for sent in proiel_dev:\n",
    "    doc = nlp_1(sent[0])\n",
    "    predicted_tags = [token.pos_ for token in doc]\n",
    "    predicted_lemmata = [lemmatizer(token.text, token.pos_) for token in doc]\n",
    "    given_tags = sent[1][\"tags\"]\n",
    "    given_lemmata = sent[1][\"lemmata\"]\n",
    "    proiel_tags_correct += len([p for p, g in zip(predicted_tags, given_tags) if p ==g])\n",
    "    proiel_lemmata_correct += len([p for p, g in zip(predicted_lemmata, given_lemmata) if p ==g])\n",
    "model_test[\"proiel_tags_correct%\"] = (proiel_tags_correct / proiel_dev_len) * 100\n",
    "model_test[\"proiel_lemmata_correct%\"] = (proiel_lemmata_correct / proiel_dev_len) * 100\n",
    "\n",
    "perseus_tags_correct = 0\n",
    "perseus_lemmata_correct = 0\n",
    "for sent in perseus_dev:\n",
    "    doc = nlp_1(sent[0])\n",
    "    predicted_tags = [token.pos_ for token in doc]\n",
    "    predicted_lemmata = [lemmatizer(token.text, token.pos_) for token in doc]\n",
    "    given_tags = sent[1][\"tags\"]\n",
    "    given_lemmata = sent[1][\"lemmata\"]\n",
    "    perseus_tags_correct += len([p for p, g in zip(predicted_tags, given_tags) if p ==g])\n",
    "    perseus_lemmata_correct += len([p for p, g in zip(predicted_lemmata, given_lemmata) if p ==g])\n",
    "model_test[\"perseus_tags_correct%\"] = (perseus_tags_correct / perseus_dev_len) * 100\n",
    "model_test[\"perseus_lemmata_correct%\"] = (perseus_lemmata_correct / perseus_dev_len) * 100\n",
    "\n",
    "model_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our results with first model:\n",
    "```\n",
    "{'proiel_tags_correct%': 87.90653384119544,\n",
    " 'proiel_lemmata_correct%': 85.62115440961031,\n",
    " 'perseus_tags_correct%': 85.05534221820646,\n",
    " 'perseus_lemmata_correct%': 82.66997967020556}\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nlp.pipe_names\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(\"../models/spacy_grc_model_6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGT = sddk.read_file(\"SDAM_data/AGT/AGT_preprocessed_20201127.json\", \"df\", conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"../models/spacy_grc_model_6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(grc_doc_lemmatizer, \"grc_doc_lemmatizer\", after=\"tagger\") # grc_doc_lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ΠΑΥΛΟΣ', 'VERB', 'Παῦλος'), ('ἀπόστολος', 'NOUN', 'ἀπόστολος'), (',', 'PUNCT', ','), ('οὐκ', 'ADV', 'οὐ'), ('ἀπʼ', 'ADP', 'ἀπό'), ('ἀνθρώπων', 'NOUN', 'ἄνθρωπος'), ('οὐδὲ', 'CCONJ', 'οὐδέ'), ('διʼ', 'ADP', 'διά'), ('ἀνθρώπου', 'NOUN', 'ἄνθρωπος'), ('ἀλλὰ', 'CCONJ', 'ἀλλά'), ('διὰ', 'ADP', 'διά'), ('Ἰησοῦ', 'PROPN', 'Ἰησοῦς'), ('Χριστοῦ', 'PROPN', 'Χριστός'), ('καὶ', 'CCONJ', 'καί'), ('θεοῦ', 'NOUN', 'θεός'), ('πατρὸς', 'NOUN', 'πατήρ'), ('τοῦ', 'DET', 'ὁ'), ('ἐγείραντος', 'VERB', 'ἐγείρω'), ('αὐτὸν', 'PRON', 'αὐτός'), ('ἐκ', 'ADP', 'ἐκ'), ('νεκρῶν', 'NOUN', 'νεκρών'), (',', 'PUNCT', ','), ('καὶ', 'CCONJ', 'καί'), ('οἱ', 'DET', 'ὁ'), ('σὺν', 'ADP', 'σύν'), ('ἐμοὶ', 'PRON', 'ἐγώ'), ('πάντες', 'ADJ', 'πᾶς'), ('ἀδελφοί', 'NOUN', 'ἀδελφός'), (',', 'PUNCT', ','), ('ταῖς', 'DET', 'ὁ'), ('ἐκκλησίαις', 'NOUN', 'ἐκκλησία'), ('τῆς', 'DET', 'ὁ'), ('Γαλατίας', 'NOUN', 'Γαλατία'), ('·', 'PUNCT', '·')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(AGT[AGT[\"author_id\"]==\"tlg0031paul\"].iloc[3][\"sentences\"][0])\n",
    "print([(token.text, token.pos_, token.lemma_) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('grc_doc_lemmatizer', <function __main__.grc_doc_lemmatizer(doc)>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.remove_pipe(\"grc_doc_lemmatizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(\"../models/spacy_grc_model_6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's continue with testing our old functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ΠΑΥΛΟΣ', 'VERB', 'Παῦλος'), ('ἀπόστολος', 'NOUN', 'ἀπόστολος'), (',', 'PUNCT', ','), ('οὐκ', 'ADV', 'οὐ'), ('ἀπʼ', 'ADP', 'ἀπό'), ('ἀνθρώπων', 'NOUN', 'ἄνθρωπος'), ('οὐδὲ', 'CCONJ', 'οὐδέ'), ('διʼ', 'ADP', 'διά'), ('ἀνθρώπου', 'NOUN', 'ἄνθρωπος'), ('ἀλλὰ', 'CCONJ', 'ἀλλά'), ('διὰ', 'ADP', 'διά'), ('Ἰησοῦ', 'PROPN', 'Ἰησοῦς'), ('Χριστοῦ', 'PROPN', 'Χριστός'), ('καὶ', 'CCONJ', 'καί'), ('θεοῦ', 'NOUN', 'θεός'), ('πατρὸς', 'NOUN', 'πατήρ'), ('τοῦ', 'DET', 'ὁ'), ('ἐγείραντος', 'VERB', 'ἐγείρω'), ('αὐτὸν', 'PRON', 'αὐτός'), ('ἐκ', 'ADP', 'ἐκ'), ('νεκρῶν', 'ADJ', 'νεκρός'), (',', 'PUNCT', ','), ('καὶ', 'CCONJ', 'καί'), ('οἱ', 'DET', 'ὁ'), ('σὺν', 'ADP', 'σύν'), ('ἐμοὶ', 'PRON', 'ἐγώ'), ('πάντες', 'ADJ', 'πᾶς'), ('ἀδελφοί', 'NOUN', 'ἀδελφός'), (',', 'PUNCT', ','), ('ταῖς', 'DET', 'ὁ'), ('ἐκκλησίαις', 'NOUN', 'ἐκκλησία'), ('τῆς', 'DET', 'ὁ'), ('Γαλατίας', 'NOUN', 'Γαλατία'), ('·', 'PUNCT', '·')]\n"
     ]
    }
   ],
   "source": [
    "doc = update_lemmata(nlp(AGT[AGT[\"author_id\"]==\"tlg0031paul\"].iloc[3][\"sentences\"][0]))\n",
    "print([(token.text, token.pos_, token.lemma_) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('πᾶσα', 'DET', 'πᾶς'), ('τέχνη', 'NOUN', 'τέχνη'), ('καὶ', 'CCONJ', 'καί'), ('πᾶσα', 'DET', 'πᾶς'), ('μέθοδος', 'NOUN', 'μέθοδος'), (',', 'PUNCT', ','), ('ὁμοίως', 'ADV', 'ὁμοίως'), ('δὲ', 'PART', 'δέ'), ('πρᾶξίς', 'ADJ', 'πρᾶξις'), ('τε', 'PART', 'τε'), ('καὶ', 'CCONJ', 'καί'), ('προαίρεσις', 'NOUN', 'προαίρεσις'), (',', 'PUNCT', ','), ('ἀγαθοῦ', 'ADJ', 'ἀγαθός'), ('τινὸς', 'PRON', 'τίς'), ('ἐφίεσθαι', 'VERB', 'ἐφίημι'), ('δοκεῖ', 'VERB', 'δοκέω'), ('·', 'PUNCT', '·')]\n",
      "[('διὸ', 'ADV', 'διό'), ('καλῶς', 'ADV', 'καλῶς'), ('ἀπεφήναντο', 'VERB', 'ἀποφαίνω'), ('τἀγαθόν', 'NOUN', 'ἀγαθός'), (',', 'PUNCT', ','), ('οὗ', 'PRON', 'ὅς'), ('πάντʼ', 'ADJ', 'πάντʼ'), ('ἐφίεται', 'VERB', 'ἐφίημι'), ('.', 'PUNCT', '.')]\n",
      "[('διαφορὰ', 'NOUN', 'διαφορά'), ('δέ', 'ADV', 'δέ'), ('τις', 'ADJ', 'τὶς'), ('φαίνεται', 'VERB', 'φαίνω'), ('τῶν', 'DET', 'ὁ'), ('τελῶν', 'NOUN', 'τέλος'), ('·', 'PUNCT', '·')]\n",
      "[('τὰ', 'DET', 'ὁ'), ('μὲν', 'PART', 'μέν'), ('γάρ', 'PART', 'γάρ'), ('εἰσιν', 'VERB', 'εἰμί'), ('ἐνέργειαι', 'NOUN', 'ἐνέργεια'), (',', 'PUNCT', ','), ('τὰ', 'DET', 'ὁ'), ('δὲ', 'CCONJ', 'δέ'), ('παρʼ', 'ADP', 'πάρʼ'), ('αὐτὰς', 'PRON', 'αὐτός'), ('ἔργα', 'NOUN', 'ἔργον'), ('τινά', 'PRON', 'τις'), ('.', 'PUNCT', '.')]\n",
      "[('ὧν', 'PRON', 'ὅς'), ('δʼ', 'ADV', 'δʼ'), ('εἰσὶ', 'AUX', 'εἰμί'), ('τέλη', 'NOUN', 'τέλος'), ('τινὰ', 'DET', 'τὶς'), ('παρὰ', 'ADP', 'παρά'), ('τὰς', 'DET', 'ὁ'), ('πράξεις', 'NOUN', 'πρᾶξις'), (',', 'PUNCT', ','), ('ἐν', 'ADP', 'ἐν'), ('τούτοις', 'PRON', 'οὗτος'), ('βελτίω', 'ADJ', 'βελτίων'), ('πέφυκε', 'VERB', 'φύω'), ('τῶν', 'DET', 'ὁ'), ('ἐνεργειῶν', 'ADJ', 'ἐνέργεια'), ('τὰ', 'DET', 'ὁ'), ('ἔργα', 'NOUN', 'ἔργον'), ('.', 'PUNCT', '.')]\n",
      "[('πολλῶν', 'ADJ', 'πολύς'), ('δὲ', 'ADV', 'δέ'), ('πράξεων', 'NOUN', 'πρᾶξις'), ('οὐσῶν', 'AUX', 'εἰμί'), ('καὶ', 'CCONJ', 'καί'), ('τεχνῶν', 'NOUN', 'τέχνη'), ('καὶ', 'CCONJ', 'καί'), ('ἐπιστημῶν', 'NOUN', 'ἐπιστήμη'), ('πολλὰ', 'ADJ', 'πολύς'), ('γίνεται', 'VERB', 'γίγνομαι'), ('καὶ', 'CCONJ', 'καί'), ('τὰ', 'DET', 'ὁ'), ('τέλη', 'NOUN', 'τέλος'), ('·', 'PUNCT', '·')]\n",
      "[('ἰατρικῆς', 'NOUN', 'ἰατρικός'), ('μὲν', 'PART', 'μέν'), ('γὰρ', 'PART', 'γάρ'), ('ὑγίεια', 'NOUN', 'ὑγίεια'), (',', 'PUNCT', ','), ('ναυπηγικῆς', 'VERB', 'ναυπηγικός'), ('δὲ', 'PART', 'δέ'), ('πλοῖον', 'NOUN', 'πλοῖον'), (',', 'PUNCT', ','), ('στρατηγικῆς', 'ADJ', 'στρατηγικός'), ('δὲ', 'PART', 'δέ'), ('νίκη', 'NOUN', 'νίκη'), (',', 'PUNCT', ','), ('οἰκονομικῆς', 'ADJ', 'οἰκονομικός'), ('δὲ', 'PART', 'δέ'), ('πλοῦτος', 'NOUN', 'πλοῦτος'), ('.', 'PUNCT', '.')]\n",
      "[('ὅσαι', 'ADJ', 'ὅσος'), ('δʼ', 'ADV', 'δʼ'), ('εἰσὶ', 'AUX', 'εἰμί'), ('τῶν', 'DET', 'ὁ'), ('τοιούτων', 'ADJ', 'τοιοῦτος'), ('ὑπὸ', 'ADP', 'ὑπό'), ('μίαν', 'NUM', 'εἷς'), ('τινὰ', 'DET', 'τὶς'), ('δύναμιν', 'NOUN', 'δύναμις'), (',', 'PUNCT', ','), ('καθάπερ', 'SCONJ', 'καθά'), ('ὑπὸ', 'ADP', 'ὑπό'), ('τὴν', 'DET', 'ὁ'), ('ἱππικὴν', 'NOUN', 'ἱππικός'), ('χαλινοποιικὴ', 'NOUN', 'χαλινοποιική'), ('καὶ', 'CCONJ', 'καί'), ('ὅσαι', 'ADJ', 'ὅσος'), ('ἄλλαι', 'DET', 'ἄλλος'), ('τῶν', 'DET', 'ὁ'), ('ἱππικῶν', 'ADJ', 'ἱππικός'), ('ὀργάνων', 'NOUN', 'ὄργανον'), ('εἰσίν', 'VERB', 'εἰμί'), (',', 'PUNCT', ','), ('αὕτη', 'ADJ', 'οὗτος'), ('δὲ', 'CCONJ', 'δέ'), ('καὶ', 'ADV', 'καί'), ('πᾶσα', 'DET', 'πᾶς'), ('πολεμικὴ', 'ADJ', 'πολεμικός'), ('πρᾶξις', 'NOUN', 'πρᾶξις'), ('ὑπὸ', 'ADP', 'ὑπό'), ('τὴν', 'DET', 'ὁ'), ('στρατηγικήν', 'NOUN', 'στρατηγικός'), (',', 'PUNCT', ','), ('κατὰ', 'ADP', 'κατά'), ('τὸν', 'DET', 'ὁ'), ('αὐτὸν', 'PRON', 'αὐτός'), ('δὴ', 'ADV', 'δή'), ('τρόπον', 'NOUN', 'τρόπος'), ('ἄλλαι', 'ADJ', 'ἄλλος'), ('ὑφʼ', 'ADP', 'ὕφʼ'), ('ἑτέρας', 'ADJ', 'ἕτερος'), ('·', 'PUNCT', '·')]\n",
      "[('ἐν', 'ADP', 'ἐν'), ('ἁπάσαις', 'ADJ', 'ἅπας'), ('δὲ', 'ADV', 'δέ'), ('τὰ', 'DET', 'ὁ'), ('τῶν', 'DET', 'ὁ'), ('ἀρχιτεκτονικῶν', 'VERB', 'ἀρχιτεκτονικών'), ('τέλη', 'NOUN', 'τέλος'), ('πάντων', 'DET', 'πᾶς'), ('ἐστὶν', 'AUX', 'εἰμί'), ('αἱρετώτερα', 'ADJ', 'αἱρετός'), ('τῶν', 'DET', 'ὁ'), ('ὑπʼ', 'ADP', 'ὕπʼ'), ('αὐτά', 'PRON', 'αὐτός'), ('·', 'PUNCT', '·')]\n",
      "[('τούτων', 'ADJ', 'οὗτος'), ('γὰρ', 'PART', 'γάρ'), ('χάριν', 'NOUN', 'χάρις'), ('κἀκεῖνα', 'ADJ', 'ἐκεῖνος'), ('διώκεται', 'VERB', 'διώκω'), ('.', 'PUNCT', '.')]\n"
     ]
    }
   ],
   "source": [
    "arist_test = apply_nlp(AGT[AGT[\"doc_id\"]==\"tlg0086.tlg010\"][\"sentences\"].tolist()[0])\n",
    "for doc in arist_test[:10]:\n",
    "    print([(token.text, token.pos_, token.lemma_) for token in doc])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
